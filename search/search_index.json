{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Docs2KG","text":"<p>A Human-LLM Collaborative Approach to Unified Knowledge Graph Construction from Heterogeneous Documents</p> <p> </p>"},{"location":"#installation","title":"Installation","text":"<p>We have published the package to PyPi: Docs2KG,</p> <p>You can install it via:</p> <pre><code>pip install Docs2KG\n\npython -m spacy download en_core_web_sm\n</code></pre> <p>Detailed setup and tutorial can be found in the documentation.</p> <p>You have two ways to run the package:</p> <ul> <li>import the package in the code, and hook it with your own code</li> <li>run the package in the command line</li> </ul>"},{"location":"#command-line","title":"Command Line","text":"<pre><code># first setup the CONFIG_FILE environment variable to local one\nexport CONFIG_FILE=config.yml # or any other path for the configuration file\n\ndocs2kg # this command will tell you how to use the package\n\n# we currently support the following commands\ndocs2kg process-document your_input_file --agent-name phi3.5 --agent-type ollama --project-id your_project_id\ndocs2kg batch-process your_input_dir --agent-name phi3.5 --agent-type ollama --project-id your_project_id\ndocs2kg list-formats # list all the supported formats\n</code></pre> <pre><code>Usage: docs2kg [OPTIONS] COMMAND [ARGS]...\n\n  Docs2KG - Document to Knowledge Graph conversion tool.\n\n  Supports multiple document formats: PDF, DOCX, HTML, and EPUB.\n\nOptions:\n  -c, --config PATH  Path to the configuration file (default: ./config.yml)\n  --help             Show this message and exit.\n\nCommands:\n  batch-process     Process all supported documents in a directory.\n  list-formats      List all supported document formats.\n  neo4j             Load data to Neo4j database.\n  process-document  Process a single document file.\n</code></pre> <pre><code>Usage: docs2kg process-document [OPTIONS] FILE_PATH\n\n  Process a single document file.\n\n  FILE_PATH: Path to the document file (PDF, DOCX, HTML, or EPUB)\n\nOptions:\n  -p, --project-id TEXT  Project ID for the knowledge graph construction\n  -n, --agent-name TEXT  Name of the agent to use for NER extraction\n  -t, --agent-type TEXT  Type of the agent to use for NER extraction\n  --help                 Show this message and exit.\n</code></pre> <pre><code>Usage: docs2kg neo4j [OPTIONS] PROJECT_ID\n\n  Load data to Neo4j database.\n\nOptions:\n  -m, --mode [import|export|load|docker_start|docker_stop]\n                                  Mode of operation (import or export)\n  -u, --neo4j-uri TEXT            URI for the Neo4j database\n  -U, --neo4j-user TEXT           Username for the Neo4j database\n  -P, --neo4j-password TEXT       Password for the Neo4j database\n  -r, --reset_db                  Reset the database before loading data\n  --help      \n</code></pre>"},{"location":"#motivation","title":"Motivation","text":"<p>To digest diverse unstructured documents into a unified knowledge graph, there are two main challenges:</p> <ul> <li>How to get the documents to be digitized?<ul> <li>With the dual-path data processing<ul> <li>For image based documents, like scanned PDF, images, etc., we can process them through the layout analysis and   OCR, etc. Docling and MinerU are focusing on this part.</li> <li>For native digital documents, like ebook, docx, html, etc., we can process them through the programming parser</li> </ul> </li> <li>It is promising that we will have a robust solution soon.</li> </ul> </li> <li>How to construct a high-quality unified knowledge graph with less effort?</li> </ul> <p>For now, a lot of tools are focusing on the first challenge, however, overlook the second challenge.</p> <p>To construct a high-quality unified knowledge graph with less effort, we propose the Docs2KG.</p> <ul> <li>We adapt both bottom-up and top-down approaches to construct the unified knowledge graph and its ontology with the   help of LLM.</li> <li>We organise the knowledge graph from three aspects:<ul> <li>MetaKG: the knowledge about all documents, like the author, the publication date, etc.</li> <li>LayoutKG: the knowledge about the layout of the documents, like title, subtitle, section, etc.</li> <li>SemanticKG: the knowledge about the content of the documents, like entities, relations, etc.</li> </ul> </li> <li>We provide a human-LLM collaborative interface which allows human to review and enhance the generated knowledge graph.<ul> <li>An updated version of ontology, entity list, relation list will in return help the KG Construction LLM agent to   generate better results in the next iteration.</li> <li>The output of the knowledge graph can be used in downstream applications, like RAG, etc.</li> <li>Link for the human-LLM collaborative interface: Docs2KG</li> <li>After the annotation, metrics to evaluate the quality of automatic construction will be provided.<ul> <li>How many entities are correctly extracted by each method?</li> <li>How many relations are correctly extracted by each method?</li> <li>Contribution and retention of each method in the final knowledge graph, including human annotation.</li> </ul> </li> </ul> </li> </ul> <p>Example of the interface, you only need to register, and you can access it freely.</p> <p> </p>"},{"location":"#setup-and-development","title":"Setup and Development","text":"<pre><code>python3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\npip install -r requirements.dev.txt\n\npip install -e .\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>If you find this package useful, please consider citing our work:</p> <pre><code>@misc{sun2024docs2kg,\n    title = {Docs2KG: Unified Knowledge Graph Construction from Heterogeneous Documents Assisted by Large Language Models},\n    author = {Qiang Sun and Yuanyi Luo and Wenxiao Zhang and Sirui Li and Jichunyang Li and Kai Niu and Xiangrui Kong and Wei Liu},\n    year = {2024},\n    eprint = {2406.02962},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n</code></pre>"},{"location":"Video/","title":"Video Demo","text":""},{"location":"Tutorial/1.GettingStarted/","title":"How to get started with Docs2KG?","text":""},{"location":"Tutorial/1.GettingStarted/#installation","title":"Installation","text":"<p>We have published the package to PyPi: Docs2KG,</p> <p>You can install it via:</p> <pre><code>pip install Docs2KG\n</code></pre>"},{"location":"Tutorial/1.GettingStarted/#confirm-the-installation","title":"Confirm the Installation","text":"<p>Run python in your terminal, and then import the package:</p> <pre><code>import Docs2KG\n\nprint(Docs2KG.__author__)  # ai4wa\n\n</code></pre>"},{"location":"Tutorial/1.GettingStarted/#set-up-the-configyml-file","title":"Set up the <code>config.yml</code> file","text":"<p>We have several configurations that you will need to set up in the <code>config.yml</code> file.</p> <p>You can copy the <code>config.example.yml</code> file and rename it to <code>config.yml</code>.</p> <pre><code>cp config.example.yml config.yml\n</code></pre> <p>There are three main configurations that you need to set up:</p> <ol> <li><code>data</code> directories for input, output and ontology</li> <li>agent configurations for <code>openai</code>, <code>ollama</code>, <code>huggingface</code> and <code>llamacpp</code></li> <li>ontology configurations regarding: entity list, relation list, ontology json and domain description text</li> </ol>"},{"location":"Tutorial/1.GettingStarted/#setup-config_file-environment-variable","title":"Setup <code>CONFIG_FILE</code> environment variable","text":"<p>You can either set the <code>CONFIG_FILE</code> environment variable to the path of the <code>config.yml</code> file.</p> <pre><code>export CONFIG_FILE=/path/to/config.yml\n</code></pre> <p>Or you can set it before you import Docs2KG in the code like this</p> <pre><code>import os\nfrom pathlib import Path\n\nos.environ[\"CONFIG_FILE\"] = str(Path.cwd() / \"config.yml\")\n</code></pre>"},{"location":"Tutorial/1.GettingStarted/#data-directories","title":"Data Directories","text":"<ol> <li><code>input_dir</code>: The directory where the input documents are stored.</li> <li><code>output_dir</code>: The directory where the output knowledge graph will be stored. Our output json will be stored in this    directory.</li> <li><code>ontology_dir</code>: The directory where the ontology files are stored.</li> </ol>"},{"location":"Tutorial/1.GettingStarted/#agent-configurations","title":"Agent Configurations","text":"<ol> <li><code>openai</code>: The configuration for the OpenAI API. You can change the endpoints to use Azure OpenAI API.</li> <li><code>ollama</code>: The configuration for the OLLAMA API. You will need to set up ollama sever by yourself and get the specific    model running.</li> <li><code>huggingface</code>: The configuration for the Huggingface API. You will need to acquire the API key from Huggingface.</li> <li><code>llamacpp</code>: The configuration for the quantization model. You will need to download your specific model and set the    path to the model.</li> </ol>"},{"location":"Tutorial/1.GettingStarted/#ontology-configurations","title":"Ontology Configurations","text":""},{"location":"Tutorial/1.GettingStarted/#entity-list-and-relation-list","title":"Entity List and Relation List","text":"<p><code>entity_list</code>: The list of entities that you want to extract from the documents. It should have the following format:</p> <pre><code>entity,entity_type\nxx,xx\n</code></pre> <p><code>relation_list</code>: The list of relations that you want to extract from the documents. It should have the following format:</p> <pre><code>relation,relation_type\nxx,xx\n</code></pre>"},{"location":"Tutorial/1.GettingStarted/#ontology-json","title":"Ontology JSON","text":"<p><code>ontology_json</code>: The ontology json file that you want to use for the knowledge graph. It should have the following format:</p> <pre><code>{\n  \"entity_types\": [\n    \"TECTSETTIN\",\n    \"Exploration Method\",\n    \"SUPERBASIN\"\n  ],\n  \"relation_types\": [\n    \"WorksFor\",\n    \"Manages\",\n    \"LeadsProject\",\n    \"BelongsTo\"\n  ],\n  \"connections\": [\n    [\n      \"WorksFor\",\n      \"Person\",\n      \"Organization\"\n    ],\n    [\n      \"Manages\",\n      \"Person\",\n      \"Department\"\n    ],\n    [\n      \"LeadsProject\",\n      \"Person\",\n      \"Project\"\n    ],\n    [\n      \"BelongsTo\",\n      \"Project\",\n      \"Department\"\n    ]\n  ]\n}\n</code></pre>"},{"location":"Tutorial/1.GettingStarted/#domain-description-text","title":"Domain Description Text","text":"<p>Put the domain description text in the <code>domain_description.txt</code> file. It will be used to generate the ontology later.</p>"},{"location":"Tutorial/2.HowToUseDocs2KGPackage/","title":"How to use Docs2KG Package?","text":"<p>After you have done the setup. Then you can start to implement your own code to generate the knowledge graph from your documents.</p> <p>The overall steps includes:</p> <ul> <li>Document Digitization: Get any format into markdown</li> <li>KG Construction: Construct the knowledge graph from the documents</li> </ul>"},{"location":"Tutorial/2.HowToUseDocs2KGPackage/#example-code","title":"Example code","text":"<pre><code>import os\nfrom pathlib import Path\n\n# set the environment variable to the config file to current directory\n\nos.environ[\"CONFIG_FILE\"] = str(Path.cwd() / \"config.yml\")\n\nfrom Docs2KG.digitization.image.pdf_docling import PDFDocling\nfrom Docs2KG.kg_construction.layout_kg.layout_kg import LayoutKGConstruction\nfrom Docs2KG.kg_construction.semantic_kg.ner.ner_spacy_match import NERSpacyMatcher\nfrom Docs2KG.utils.config import PROJECT_CONFIG\n\nif __name__ == \"__main__\":\n    # digitization for PDF\n    pdf_path = (\n            PROJECT_CONFIG.data.input_dir / \"gsdRec_2024_08.pdf\"\n    )  # path to the pdf file or any other format\n    processor = PDFDocling(file_path=pdf_path)  # initialize the processor\n    processor.process()  # process the pdf file\n    # knowledge graph construction\n    project_id = \"wamex\"  # project id\n    md_files = (\n            PROJECT_CONFIG.data.output_dir\n            / \"gsdRec_2024_08\"\n            / \"PDFDocling\"\n            / \"gsdRec_2024_08.md\"\n    )  # path to the markdown file which is generated from the pdf\n    layout_kg_construction = LayoutKGConstruction(\n        project_id\n    )  # initialize the layout kg construction\n    layout_kg_construction.construct(\n        [{\"content\": md_files.read_text(), \"filename\": md_files.stem}]\n    )  # construct the layout kg\n    example_json = (\n            PROJECT_CONFIG.data.output_dir\n            / \"projects\"\n            / project_id\n            / \"layout\"\n            / \"gsdRec_2024_08.json\"\n    )  # path to the layout kg json file\n    # entity extraction with NER spacy matcher given an entity list\n    entity_extractor = NERSpacyMatcher(project_id)\n    entity_extractor.construct_kg([example_json])\n\n</code></pre> <p>After this, the output json will be with the entities and relations extracted from the documents.</p> <p>Upload the output json to the Docs2KG web interface for next step human-in-the-loop validation and verification.</p>"},{"location":"Tutorial/3.HowToUseDocs2KGInterface/","title":"How to use Docs2KG Interface?","text":"<p>It is freely accessible via: Docs2KG Interface.</p> <p>You can register freely and then login with your account.</p>"},{"location":"Tutorial/3.HowToUseDocs2KGInterface/#demo-project","title":"Demo Project","text":"<p>To view and have a try, you can see our demo public project:</p> <p></p> <p>Click it and then go to Docs2KG application.</p> <p>You will be able to view this and upload the generated json from Docs2KG package.</p> <p></p> <p>Open the uploaded file by click the 'eye' icon, you will be able to start the edit</p> <p></p> <p>You can add/remove entities and relations, and then save the changes, download the updated json file.</p> <p>After you finish the annotation, you can also view the metrics of the Doc2KG package and annotation.</p> <p></p> <p>You can also add/remove entity types and relation types in the ontology, also upload the entity list by click the setting icon.</p> <p></p>"},{"location":"Tutorial/3.HowToUseDocs2KGInterface/#your-own-project","title":"Your Own Project","text":"<p>In Dashboard, create a project, give it a name and then make it private.</p> <p>Then you can upload the json file generated from Docs2KG package.</p> <p>Do similar things as the demo project, you will be able to have a clean and clear knowledge graph.</p>"},{"location":"Tutorial/4.HowToExtendDocs2KG/","title":"How to extend Docs2KG","text":""},{"location":"Tutorial/4.HowToExtendDocs2KG/#for-the-package-side","title":"For the package side","text":"<p>If you want to add a new approach to extract relations or entities, it is very simple.</p> <p>The only one principle you will need to follow is to make sure you will update the layout.json file with proper entities and relations.</p> <p>For example:</p> <pre><code>{\n  \"filename\": \"gsdRec_2024_08\",\n  \"data\": [\n    {\n      \"id\": \"p_8ffe9562-7be9-4e00-8e09-980972612298\",\n      \"text\": \"RECORD 2024/8\",\n      \"label\": \"P\",\n      \"entities\": [\n        {\n          \"text\": \"2024/8\",\n          \"label\": \"Project Code\",\n          \"confidence\": 1.0,\n          \"start\": 7,\n          \"end\": 13,\n          \"id\": \"ner-llm--6189338238195147291\",\n          \"method\": \"NERLLMExtractor\"\n        }\n      ],\n      \"relations\": []\n    }\n  ]\n}\n</code></pre> <p>This is one of the generated layout.json files, you just need to update this file with your generated ner or relations. And extend current entity and relation list.</p> <p>To achieve that you can extend our <code>SemanticKGConstructionBase</code> class</p> <pre><code>from Docs2KG.kg_construction.semantic_kg.base import SemanticKGConstructionBase\nfrom typing import List, Dict, Any\n\n\nclass YourNewApproach(SemanticKGConstructionBase):\n    def __init__(self, project_id: str):\n        super().__init__(project_id)\n\n    def construct_kg(self, data: List[Dict[str, Any]]) -&gt; None:\n        # your code here\n        pass\n\n</code></pre>"},{"location":"Tutorial/4.HowToExtendDocs2KG/#for-the-interface-side","title":"For the interface side","text":"<p>If you want a new feature in the interface, all you need to do is to email us, describe clearly what you want.</p>"},{"location":"sources/cli/","title":"Cli","text":""},{"location":"sources/cli/#Docs2KG.cli.DocumentProcessor","title":"<code>DocumentProcessor</code>","text":"Source code in <code>Docs2KG/cli.py</code> <pre><code>class DocumentProcessor:\n    PROCESSORS = {\n        \".pdf\": PDFDocling,\n        \".docx\": DOCXMammoth,\n        \".html\": HTMLDocling,\n        \".epub\": EPUBDigitization,\n    }\n\n    @classmethod\n    def get_processor(cls, file_path: Path) -&gt; Optional[Type]:\n        \"\"\"Get the appropriate processor for the file type.\"\"\"\n        return cls.PROCESSORS.get(file_path.suffix.lower())\n\n    @classmethod\n    def get_supported_formats(cls) -&gt; str:\n        \"\"\"Get a string of supported file formats.\"\"\"\n        return \", \".join(cls.PROCESSORS.keys())\n</code></pre>"},{"location":"sources/cli/#Docs2KG.cli.DocumentProcessor.get_processor","title":"<code>get_processor(file_path)</code>  <code>classmethod</code>","text":"<p>Get the appropriate processor for the file type.</p> Source code in <code>Docs2KG/cli.py</code> <pre><code>@classmethod\ndef get_processor(cls, file_path: Path) -&gt; Optional[Type]:\n    \"\"\"Get the appropriate processor for the file type.\"\"\"\n    return cls.PROCESSORS.get(file_path.suffix.lower())\n</code></pre>"},{"location":"sources/cli/#Docs2KG.cli.DocumentProcessor.get_supported_formats","title":"<code>get_supported_formats()</code>  <code>classmethod</code>","text":"<p>Get a string of supported file formats.</p> Source code in <code>Docs2KG/cli.py</code> <pre><code>@classmethod\ndef get_supported_formats(cls) -&gt; str:\n    \"\"\"Get a string of supported file formats.\"\"\"\n    return \", \".join(cls.PROCESSORS.keys())\n</code></pre>"},{"location":"sources/cli/#Docs2KG.cli.batch_process","title":"<code>batch_process(input_dir, project_id, formats, agent_name, agent_type)</code>","text":"<p>Process all supported documents in a directory.</p> <p>INPUT_DIR: Directory containing documents to process</p> Source code in <code>Docs2KG/cli.py</code> <pre><code>@cli.command()\n@click.argument(\n    \"input_dir\",\n    type=click.Path(exists=True),\n    default=PROJECT_CONFIG.data.input_dir.as_posix(),\n)\n@click.option(\n    \"--project-id\",\n    \"-p\",\n    default=\"default\",\n    help=\"Project ID for the knowledge graph construction\",\n)\n@click.option(\n    \"--formats\",\n    \"-f\",\n    help='Comma-separated list of file formats to process (e.g., \"pdf,docx,html\")',\n)\n@click.option(\n    \"--agent-name\",\n    \"-n\",\n    default=\"phi3.5\",\n    help=\"Name of the agent to use for NER extraction\",\n)\n@click.option(\n    \"--agent-type\",\n    \"-t\",\n    default=\"ollama\",\n    help=\"Type of the agent to use for NER extraction\",\n)\ndef batch_process(input_dir, project_id, formats, agent_name, agent_type):\n    \"\"\"Process all supported documents in a directory.\n\n    INPUT_DIR: Directory containing documents to process\n    \"\"\"\n    input_dir = Path(input_dir)\n\n    # Filter formats if specified\n    if formats:\n        allowed_formats = {f\".{fmt.lower().strip()}\" for fmt in formats.split(\",\")}\n        supported_formats = set(DocumentProcessor.PROCESSORS.keys())\n        invalid_formats = allowed_formats - supported_formats\n        if invalid_formats:\n            raise click.ClickException(\n                f\"Unsupported format(s): {', '.join(invalid_formats)}. \"\n                f\"Supported formats are: {DocumentProcessor.get_supported_formats()}\"\n            )\n    else:\n        allowed_formats = set(DocumentProcessor.PROCESSORS.keys())\n\n    # Find all files with supported extensions\n    files_to_process = []\n    for ext in allowed_formats:\n        files_to_process.extend(input_dir.glob(f\"*{ext}\"))\n\n    if not files_to_process:\n        logger.warning(\n            f\"No supported documents found in {input_dir}. \"\n            f\"Looking for: {', '.join(allowed_formats)}\"\n        )\n        return\n\n    logger.info(f\"Found {len(files_to_process)} documents to process\")\n\n    for file_path in files_to_process:\n        try:\n            process_single_file(file_path, project_id, agent_name, agent_type)\n        except Exception as e:\n            logger.error(f\"Error processing {file_path.name}: {str(e)}\")\n            continue\n\n    logger.info(\"Batch processing completed\")\n</code></pre>"},{"location":"sources/cli/#Docs2KG.cli.cli","title":"<code>cli()</code>","text":"<p>Docs2KG - Document to Knowledge Graph conversion tool.</p> <p>Supports multiple document formats: PDF, DOCX, HTML, and EPUB.</p> Source code in <code>Docs2KG/cli.py</code> <pre><code>@click.group()\ndef cli():\n    \"\"\"Docs2KG - Document to Knowledge Graph conversion tool.\n\n    Supports multiple document formats: PDF, DOCX, HTML, and EPUB.\n    \"\"\"\n    logger.info(\"Welcome to Docs2KG!\")\n    logger.info(f\"Configuration loaded from: {os.environ.get('CONFIG_FILE')}\")\n    logger.info(f\"Input directory: {PROJECT_CONFIG.data.input_dir}\")\n    logger.info(f\"Output directory: {PROJECT_CONFIG.data.output_dir}\")\n    logger.info(f\"Ontology directory: {PROJECT_CONFIG.data.ontology_dir}\")\n    logger.info(\"---\")\n</code></pre>"},{"location":"sources/cli/#Docs2KG.cli.list_formats","title":"<code>list_formats()</code>","text":"<p>List all supported document formats.</p> Source code in <code>Docs2KG/cli.py</code> <pre><code>@cli.command()\ndef list_formats():\n    \"\"\"List all supported document formats.\"\"\"\n    supported_formats = DocumentProcessor.get_supported_formats()\n    click.echo(f\"Supported document formats: {supported_formats}\")\n</code></pre>"},{"location":"sources/cli/#Docs2KG.cli.neo4j","title":"<code>neo4j(project_id, mode, neo4j_uri, neo4j_user, neo4j_password, reset_db)</code>","text":"<p>Load data to Neo4j database.</p> Source code in <code>Docs2KG/cli.py</code> <pre><code>@cli.command()\n@click.argument(\"project_id\", type=str)\n# import or export mode\n@click.option(\n    \"--mode\",\n    \"-m\",\n    type=click.Choice([\"import\", \"export\", \"load\", \"docker_start\", \"docker_stop\"]),\n    default=\"load\",\n    help=\"Mode of operation (import or export)\",\n)\n@click.option(\n    \"--neo4j-uri\",\n    \"-u\",\n    default=\"bolt://localhost:7687\",\n    help=\"URI for the Neo4j database\",\n)\n@click.option(\n    \"--neo4j-user\",\n    \"-U\",\n    default=\"neo4j\",\n    help=\"Username for the Neo4j database\",\n)\n@click.option(\n    \"--neo4j-password\",\n    \"-P\",\n    default=\"testpassword\",\n    help=\"Password for the Neo4j database\",\n)\n@click.option(\n    \"--reset_db\",\n    \"-r\",\n    is_flag=True,\n    help=\"Reset the database before loading data\",\n    default=False,\n)\ndef neo4j(project_id, mode, neo4j_uri, neo4j_user, neo4j_password, reset_db):\n    \"\"\"Load data to Neo4j database.\"\"\"\n    input_path = PROJECT_CONFIG.data.output_dir / \"projects\" / project_id / \"layout\"\n    logger.info(f\"Processing input: {input_path}\")\n\n    if mode == \"docker_start\":\n        docker_compose = \"\"\"\nservices:\n  neo4j:\n    image: neo4j:latest\n    container_name: neo4j\n    environment:\n      - NEO4J_AUTH=neo4j/testpassword\n      - NEO4JLABS_PLUGINS=[\"apoc\"]\n      - NEO4J_apoc_import_file_enabled=true\n      - NEO4J_apoc_export_file_enabled=true\n      - NEO4J_dbms_security_procedures_unrestricted=apoc.*\n    ports:\n      - 7474:7474\n      - 7687:7687\n    volumes:\n      - neo4j_data:/data\n      - neo4j_logs:/logs\n      - neo4j_import:/import\n      - neo4j_plugins:/plugins\n\nvolumes:\n  neo4j_data:\n  neo4j_logs:\n  neo4j_import:\n  neo4j_plugins:\n    \"\"\"\n\n        # Write docker-compose file\n        with open(PROJECT_CONFIG.data.output_dir / \"docker-compose.yml\", \"w\") as f:\n            f.write(docker_compose)\n\n        logger.info(\"Created docker-compose.yml file\")\n\n        try:\n            # Try using docker compose first (newer syntax)\n            subprocess.run(\n                [\n                    \"docker\",\n                    \"compose\",\n                    \"-f\",\n                    (PROJECT_CONFIG.data.output_dir / \"docker-compose.yml\").as_posix(),\n                    \"up\",\n                    \"-d\",\n                ],\n                check=True,\n            )\n        except subprocess.CalledProcessError:\n            try:\n                # Fall back to docker-compose if the above fails\n                subprocess.run(\n                    [\n                        \"docker-compose\",\n                        \"-f\",\n                        (\n                            PROJECT_CONFIG.data.output_dir / \"docker-compose.yml\"\n                        ).as_posix(),\n                        \"up\",\n                        \"-d\",\n                    ],\n                    check=True,\n                )\n            except subprocess.CalledProcessError as e:\n                logger.error(\"Failed to start Docker container: %s\", e)\n                raise\n\n        logger.info(\"Neo4j container is starting up\")\n        logger.info(\"Access Neo4j Browser at http://localhost:7474\")\n        return\n\n    if mode == \"docker_stop\":\n        try:\n            # Try using docker compose first (newer syntax)\n            subprocess.run(\n                [\n                    \"docker\",\n                    \"compose\",\n                    \"-f\",\n                    (PROJECT_CONFIG.data.output_dir / \"docker-compose.yml\").as_posix(),\n                    \"down\",\n                ],\n                check=True,\n            )\n        except subprocess.CalledProcessError:\n            try:\n                # Fall back to docker-compose if the above fails\n                subprocess.run(\n                    [\n                        \"docker-compose\",\n                        \"-f\",\n                        (\n                            PROJECT_CONFIG.data.output_dir / \"docker-compose.yml\"\n                        ).as_posix(),\n                        \"down\",\n                    ],\n                    check=True,\n                )\n            except subprocess.CalledProcessError as e:\n                logger.error(\"Failed to stop Docker container: %s\", e)\n                raise\n\n        logger.info(\"Neo4j container is stopping\")\n        return\n\n    # Initialize Neo4j transformer\n    transformer = Neo4jTransformer(\n        project_id=project_id,\n        uri=neo4j_uri,\n        username=neo4j_user,\n        password=neo4j_password,\n        reset_database=reset_db,\n    )\n\n    if mode == \"load\":\n        if input_path.is_dir():\n            for file_path in input_path.glob(\"*.json\"):\n                # if it is schema.json, skip\n                if file_path.name == \"schema.json\":\n                    continue\n                try:\n                    transformer.transform_and_load(file_path)\n                except Exception as e:\n                    logger.error(f\"Error loading {file_path.name}: {str(e)}\")\n                    logger.exception(e)\n                    continue\n        else:\n            try:\n                transformer.transform_and_load(input_path)\n            except Exception as e:\n                logger.error(f\"Error loading {input_path.name}: {str(e)}\")\n\n    elif mode == \"export\":\n        transformer.export()\n    elif mode == \"import\":\n        project_json_path = (\n            PROJECT_CONFIG.data.output_dir\n            / \"projects\"\n            / project_id\n            / \"neo4j_export.json\"\n        )\n        transformer.import_from_json(project_json_path)\n    else:\n        raise click.ClickException(\"Invalid mode of operation\")\n</code></pre>"},{"location":"sources/cli/#Docs2KG.cli.process_document","title":"<code>process_document(file_path, project_id, agent_name, agent_type)</code>","text":"<p>Process a single document file.</p> <p>FILE_PATH: Path to the document file (PDF, DOCX, HTML, or EPUB)</p> Source code in <code>Docs2KG/cli.py</code> <pre><code>@cli.command()\n@click.argument(\"file_path\", type=click.Path(exists=True))\n@click.option(\n    \"--project-id\",\n    \"-p\",\n    default=\"default\",\n    help=\"Project ID for the knowledge graph construction\",\n)\n@click.option(\n    \"--agent-name\",\n    \"-n\",\n    default=\"phi3.5\",\n    help=\"Name of the agent to use for NER extraction\",\n)\n@click.option(\n    \"--agent-type\",\n    \"-t\",\n    default=\"ollama\",\n    help=\"Type of the agent to use for NER extraction\",\n)\ndef process_document(file_path, project_id, agent_name, agent_type):\n    \"\"\"Process a single document file.\n\n    FILE_PATH: Path to the document file (PDF, DOCX, HTML, or EPUB)\n    \"\"\"\n    file_path = Path(file_path)\n    logger.info(f\"Processing document: {file_path}\")\n    process_single_file(file_path, project_id, agent_name, agent_type)\n</code></pre>"},{"location":"sources/cli/#Docs2KG.cli.process_single_file","title":"<code>process_single_file(file_path, project_id, agent_name, agent_type)</code>","text":"<p>Process a single document file.</p> Source code in <code>Docs2KG/cli.py</code> <pre><code>def process_single_file(\n    file_path: Path, project_id: str, agent_name: str, agent_type: str\n):\n    \"\"\"Process a single document file.\"\"\"\n    processor_class = DocumentProcessor.get_processor(file_path)\n    if not processor_class:\n        supported_formats = DocumentProcessor.get_supported_formats()\n        raise click.ClickException(\n            f\"Unsupported file format: {file_path.suffix}. \"\n            f\"Supported formats are: {supported_formats}\"\n        )\n\n    # Step 1: Process document\n    processor = processor_class(file_path=file_path)\n    processor.process()\n\n    # Step 2: Get markdown file path\n    md_files = (\n        PROJECT_CONFIG.data.output_dir\n        / file_path.stem\n        / processor_class.__name__\n        / f\"{file_path.stem}.md\"\n    )\n\n    if not md_files.exists():\n        logger.error(f\"Markdown file not found: {md_files}\")\n        raise click.ClickException(\"Document processing failed\")\n\n    # Step 3: Construct Layout KG\n    layout_kg_construction = LayoutKGConstruction(project_id)\n    layout_kg_construction.construct(\n        [{\"content\": md_files.read_text(), \"filename\": md_files.stem}]\n    )\n\n    # Step 4: Get JSON file path\n    example_json = (\n        PROJECT_CONFIG.data.output_dir\n        / \"projects\"\n        / project_id\n        / \"layout\"\n        / f\"{file_path.stem}.json\"\n    )\n\n    if not example_json.exists():\n        logger.error(f\"Layout KG JSON file not found: {example_json}\")\n        raise click.ClickException(\"Layout KG construction failed\")\n\n    # Step 5: Extract entities\n    entity_extractor = NERSpacyMatcher(project_id)\n    entity_extractor.construct_kg([example_json])\n\n    # Step 6: Extract via prompt-based NER\n    ner_extractor = NERLLMPromptExtractor(\n        project_id=project_id, agent_name=agent_name, agent_type=agent_type\n    )\n    ner_extractor.construct_kg([example_json])\n\n    logger.info(f\"Successfully processed {file_path.name}\")\n</code></pre>"},{"location":"sources/agents/base/","title":"Base","text":""},{"location":"sources/agents/cloud/","title":"Cloud","text":""},{"location":"sources/agents/cloud/#Docs2KG.agents.cloud.CloudAgent","title":"<code>CloudAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>Docs2KG/agents/cloud.py</code> <pre><code>class CloudAgent(BaseAgent):\n    def __init__(self, name: str):\n        \"\"\"\n        Initialize CloudAgent with model name and optional API key.\n\n        Args:\n            name: Name of the model to use (e.g., 'gpt-4')\n        \"\"\"\n        super().__init__(name)\n        self.client = self._init_openai_client()\n\n    def _init_openai_client(self) -&gt; OpenAI:\n        \"\"\"\n        Initialize OpenAI client with API key from either:\n        1. Explicitly passed api_key parameter\n        2. Environment variable OPENAI_API_KEY\n        3. .env file\n        \"\"\"\n        try:\n            # Initialize client with config\n            client = OpenAI(\n                api_key=PROJECT_CONFIG.openai.api_key.get_secret_value(),\n                base_url=PROJECT_CONFIG.openai.api_base,\n                timeout=PROJECT_CONFIG.openai.timeout,\n                max_retries=PROJECT_CONFIG.openai.max_retries,\n            )\n\n            logger.info(f\"Successfully initialized OpenAI client for model {self.name}\")\n            return client\n\n        except Exception as e:\n            logger.error(f\"Failed to initialize OpenAI client: {str(e)}\")\n            raise\n\n    def process(self, input_data: Any) -&gt; Any:\n        \"\"\"\n        Process input using the OpenAI client.\n\n        Args:\n            input_data: The input to be processed by the model\n\n        Returns:\n            Dict containing the model response and metadata\n        \"\"\"\n        logger.info(f\"Processing input with OpenAI: {input_data}\")\n\n        try:\n            # Create chat completion with proper error handling\n            response = self.client.chat.completions.create(\n                model=self.name,\n                messages=[{\"role\": \"user\", \"content\": str(input_data)}],\n                max_tokens=PROJECT_CONFIG.openai.max_tokens,\n                temperature=PROJECT_CONFIG.openai.temperature,\n            )\n\n            return {\n                \"model\": self.name,\n                \"input\": input_data,\n                \"status\": \"processed\",\n                \"response\": response.choices[0].message.content,\n                \"usage\": response.usage.dict() if response.usage else None,\n            }\n\n        except Exception as e:\n            logger.error(f\"Error processing input with OpenAI: {str(e)}\")\n            raise\n</code></pre>"},{"location":"sources/agents/cloud/#Docs2KG.agents.cloud.CloudAgent.__init__","title":"<code>__init__(name)</code>","text":"<p>Initialize CloudAgent with model name and optional API key.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model to use (e.g., 'gpt-4')</p> required Source code in <code>Docs2KG/agents/cloud.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"\n    Initialize CloudAgent with model name and optional API key.\n\n    Args:\n        name: Name of the model to use (e.g., 'gpt-4')\n    \"\"\"\n    super().__init__(name)\n    self.client = self._init_openai_client()\n</code></pre>"},{"location":"sources/agents/cloud/#Docs2KG.agents.cloud.CloudAgent.process","title":"<code>process(input_data)</code>","text":"<p>Process input using the OpenAI client.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The input to be processed by the model</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Dict containing the model response and metadata</p> Source code in <code>Docs2KG/agents/cloud.py</code> <pre><code>def process(self, input_data: Any) -&gt; Any:\n    \"\"\"\n    Process input using the OpenAI client.\n\n    Args:\n        input_data: The input to be processed by the model\n\n    Returns:\n        Dict containing the model response and metadata\n    \"\"\"\n    logger.info(f\"Processing input with OpenAI: {input_data}\")\n\n    try:\n        # Create chat completion with proper error handling\n        response = self.client.chat.completions.create(\n            model=self.name,\n            messages=[{\"role\": \"user\", \"content\": str(input_data)}],\n            max_tokens=PROJECT_CONFIG.openai.max_tokens,\n            temperature=PROJECT_CONFIG.openai.temperature,\n        )\n\n        return {\n            \"model\": self.name,\n            \"input\": input_data,\n            \"status\": \"processed\",\n            \"response\": response.choices[0].message.content,\n            \"usage\": response.usage.dict() if response.usage else None,\n        }\n\n    except Exception as e:\n        logger.error(f\"Error processing input with OpenAI: {str(e)}\")\n        raise\n</code></pre>"},{"location":"sources/agents/exceptions/","title":"Exceptions","text":""},{"location":"sources/agents/exceptions/#Docs2KG.agents.exceptions.AgentError","title":"<code>AgentError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for agent-related errors</p> Source code in <code>Docs2KG/agents/exceptions.py</code> <pre><code>class AgentError(Exception):\n    \"\"\"Base exception for agent-related errors\"\"\"\n\n    pass\n</code></pre>"},{"location":"sources/agents/exceptions/#Docs2KG.agents.exceptions.ConfigurationError","title":"<code>ConfigurationError</code>","text":"<p>               Bases: <code>AgentError</code></p> <p>Raised when there's an error in agent configuration</p> Source code in <code>Docs2KG/agents/exceptions.py</code> <pre><code>class ConfigurationError(AgentError):\n    \"\"\"Raised when there's an error in agent configuration\"\"\"\n\n    pass\n</code></pre>"},{"location":"sources/agents/exceptions/#Docs2KG.agents.exceptions.InvalidAgentType","title":"<code>InvalidAgentType</code>","text":"<p>               Bases: <code>AgentError</code></p> <p>Raised when an invalid agent type is specified</p> Source code in <code>Docs2KG/agents/exceptions.py</code> <pre><code>class InvalidAgentType(AgentError):\n    \"\"\"Raised when an invalid agent type is specified\"\"\"\n\n    pass\n</code></pre>"},{"location":"sources/agents/hf/","title":"Hf","text":""},{"location":"sources/agents/hf/#Docs2KG.agents.hf.HuggingFaceAgent","title":"<code>HuggingFaceAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>Docs2KG/agents/hf.py</code> <pre><code>class HuggingFaceAgent(BaseAgent):\n    def __init__(self, name: str):\n        \"\"\"\n        Initialize HuggingFaceAgent with model name.\n\n        Args:\n            name: Name of the model to use (e.g., 'gpt2')\n        \"\"\"\n        super().__init__(name)\n        self.client = self._init_huggingface_client()\n\n    def _init_huggingface_client(self) -&gt; InferenceClient:\n        \"\"\"\n        Initialize HuggingFace client with API token from either:\n        1. Environment variable HF_API_TOKEN\n        2. .env file\n        \"\"\"\n        try:\n            # Initialize client with config\n            client = InferenceClient(\n                model=self.name,\n                token=PROJECT_CONFIG.huggingface.api_token.get_secret_value(),\n            )\n\n            logger.info(\n                f\"Successfully initialized HuggingFace client for model {self.name}\"\n            )\n            return client\n\n        except Exception as e:\n            logger.error(f\"Failed to initialize HuggingFace client: {str(e)}\")\n            raise\n\n    def process(self, input_data: Any) -&gt; Any:\n        \"\"\"\n        Process input using the HuggingFace client.\n\n        Args:\n            input_data: The input to be processed by the model\n\n        Returns:\n            Dict containing the model response and metadata\n        \"\"\"\n        logger.info(f\"Processing input with HuggingFace: {input_data}\")\n\n        try:\n            # Query the model with proper error handling\n            response = self.client.text_generation(\n                prompt=str(input_data),\n                details=True,  # Get detailed response including token counts\n                return_full_text=False,  # Only return generated text, not the prompt\n            )\n\n            return {\n                \"model\": self.name,\n                \"input\": input_data,\n                \"status\": \"processed\",\n                \"response\": response.generated_text,\n                \"usage\": {\n                    \"prompt_tokens\": len(str(input_data).split()),\n                    \"completion_tokens\": len(response.generated_text.split()),\n                    \"total_tokens\": len(str(input_data).split())\n                    + len(response.generated_text.split()),\n                },\n            }\n\n        except Exception as e:\n            logger.error(f\"Error processing input with HuggingFace: {str(e)}\")\n            raise\n</code></pre>"},{"location":"sources/agents/hf/#Docs2KG.agents.hf.HuggingFaceAgent.__init__","title":"<code>__init__(name)</code>","text":"<p>Initialize HuggingFaceAgent with model name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model to use (e.g., 'gpt2')</p> required Source code in <code>Docs2KG/agents/hf.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"\n    Initialize HuggingFaceAgent with model name.\n\n    Args:\n        name: Name of the model to use (e.g., 'gpt2')\n    \"\"\"\n    super().__init__(name)\n    self.client = self._init_huggingface_client()\n</code></pre>"},{"location":"sources/agents/hf/#Docs2KG.agents.hf.HuggingFaceAgent.process","title":"<code>process(input_data)</code>","text":"<p>Process input using the HuggingFace client.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The input to be processed by the model</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Dict containing the model response and metadata</p> Source code in <code>Docs2KG/agents/hf.py</code> <pre><code>def process(self, input_data: Any) -&gt; Any:\n    \"\"\"\n    Process input using the HuggingFace client.\n\n    Args:\n        input_data: The input to be processed by the model\n\n    Returns:\n        Dict containing the model response and metadata\n    \"\"\"\n    logger.info(f\"Processing input with HuggingFace: {input_data}\")\n\n    try:\n        # Query the model with proper error handling\n        response = self.client.text_generation(\n            prompt=str(input_data),\n            details=True,  # Get detailed response including token counts\n            return_full_text=False,  # Only return generated text, not the prompt\n        )\n\n        return {\n            \"model\": self.name,\n            \"input\": input_data,\n            \"status\": \"processed\",\n            \"response\": response.generated_text,\n            \"usage\": {\n                \"prompt_tokens\": len(str(input_data).split()),\n                \"completion_tokens\": len(response.generated_text.split()),\n                \"total_tokens\": len(str(input_data).split())\n                + len(response.generated_text.split()),\n            },\n        }\n\n    except Exception as e:\n        logger.error(f\"Error processing input with HuggingFace: {str(e)}\")\n        raise\n</code></pre>"},{"location":"sources/agents/manager/","title":"Manager","text":""},{"location":"sources/agents/manager/#Docs2KG.agents.manager.AgentManager","title":"<code>AgentManager</code>","text":"Source code in <code>Docs2KG/agents/manager.py</code> <pre><code>class AgentManager:\n    def __init__(self, agent_name: str, agent_type: str, **kwargs):\n        \"\"\"\n        Initialize AgentManager with a specific agent.\n\n        Args:\n            agent_name: Name for the agent (e.g., 'gpt-4', 'gpt-4-turbo')\n            agent_type: Type of agent ('cloud', 'quantization', or 'hf')\n        \"\"\"\n        self.agent_types = {\n            \"cloud\": CloudAgent,\n            \"quantization\": QuantizationAgent,\n            \"ollama\": OllamaAgent,\n            \"hf\": HuggingFaceAgent,\n        }\n        self.agent_type = agent_type\n\n        self.agent = self._init_agent(agent_name, agent_type, **kwargs)\n\n    def _init_agent(self, agent_name: str, agent_type: str, **kwargs) -&gt; BaseAgent:\n        agent_type = agent_type.lower()\n        if agent_type not in self.agent_types:\n            raise InvalidAgentType(\n                f\"Invalid agent type. Must be one of: {', '.join(self.agent_types.keys())}\"\n            )\n\n        agent_class = self.agent_types[agent_type]\n        return agent_class(agent_name, **kwargs)\n\n    def process_input(self, input_data: Any, reset_session: bool = False) -&gt; Any:\n        if self.agent_type == \"ollama\":\n            return self.agent.process(input_data, reset_session)\n        return self.agent.process(input_data)\n\n    def get_agent_info(self) -&gt; Dict[str, str]:\n        return {\n            \"name\": self.agent.name,\n            \"type\": type(self.agent).__name__,\n            \"config\": getattr(self.agent, \"model\", None),\n        }\n</code></pre>"},{"location":"sources/agents/manager/#Docs2KG.agents.manager.AgentManager.__init__","title":"<code>__init__(agent_name, agent_type, **kwargs)</code>","text":"<p>Initialize AgentManager with a specific agent.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>Name for the agent (e.g., 'gpt-4', 'gpt-4-turbo')</p> required <code>agent_type</code> <code>str</code> <p>Type of agent ('cloud', 'quantization', or 'hf')</p> required Source code in <code>Docs2KG/agents/manager.py</code> <pre><code>def __init__(self, agent_name: str, agent_type: str, **kwargs):\n    \"\"\"\n    Initialize AgentManager with a specific agent.\n\n    Args:\n        agent_name: Name for the agent (e.g., 'gpt-4', 'gpt-4-turbo')\n        agent_type: Type of agent ('cloud', 'quantization', or 'hf')\n    \"\"\"\n    self.agent_types = {\n        \"cloud\": CloudAgent,\n        \"quantization\": QuantizationAgent,\n        \"ollama\": OllamaAgent,\n        \"hf\": HuggingFaceAgent,\n    }\n    self.agent_type = agent_type\n\n    self.agent = self._init_agent(agent_name, agent_type, **kwargs)\n</code></pre>"},{"location":"sources/agents/ollama/","title":"Ollama","text":""},{"location":"sources/agents/ollama/#Docs2KG.agents.ollama.OllamaAgent","title":"<code>OllamaAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>Docs2KG/agents/ollama.py</code> <pre><code>class OllamaAgent(BaseAgent):\n    def __init__(self, name: str):\n        \"\"\"\n        Initialize OllamaAgent with model name and optional API base URL.\n\n        Args:\n            name: Name of the Ollama model to use (e.g., 'llama2', 'mistral')\n        \"\"\"\n        super().__init__(name)\n        self.session = self._init_session()\n\n    def _init_session(self) -&gt; requests.Session:\n        \"\"\"Initialize requests session with retries and timeouts\"\"\"\n        try:\n            session = requests.Session()\n\n            # Configure retries\n            retries = Retry(\n                total=PROJECT_CONFIG.ollama.max_retries,\n                backoff_factor=0.5,\n                status_forcelist=[500, 502, 503, 504],\n            )\n\n            # Set up the session with retry configuration\n            session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n            session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n\n            # Set base URL\n            self.api_base = PROJECT_CONFIG.ollama.api_base\n\n            logger.info(\n                f\"Successfully initialized Ollama session for model {self.name}\"\n            )\n            return session\n\n        except Exception as e:\n            logger.error(f\"Failed to initialize Ollama session: {str(e)}\")\n            raise\n\n    def reset_session(self):\n        \"\"\"Reset the requests session\"\"\"\n        self.session.close()\n        self.session = self._init_session()\n\n    def process(self, input_data: Any, reset_session: bool = False) -&gt; Any:\n        \"\"\"\n        Process input using the Ollama API.\n\n        Args:\n            input_data: The input to be processed by the model\n            reset_session: Whether to reset the requests session before making the API call\n\n        Returns:\n            Dict containing the model response and metadata\n        \"\"\"\n        logger.info(f\"Using Ollama model: {self.name}\")\n\n        try:\n            # Prepare the request\n            url = f\"{self.api_base}/api/generate\"\n\n            payload = {\n                \"model\": self.name,\n                \"prompt\": str(input_data),\n                \"temperature\": PROJECT_CONFIG.ollama.temperature,\n                \"stream\": False,\n                \"format\": PROJECT_CONFIG.ollama.format,\n            }\n\n            # Make the API call\n            if reset_session:\n                self.reset_session()\n            response = self.session.post(\n                url, json=payload, timeout=PROJECT_CONFIG.ollama.timeout\n            )\n            response.raise_for_status()\n\n            result = response.json()\n\n            return {\n                \"model\": self.name,\n                \"input\": input_data,\n                \"status\": \"processed\",\n                \"response\": result.get(\"response\", \"\"),\n                \"usage\": {\n                    \"eval_count\": result.get(\"eval_count\", 0),\n                    \"eval_duration\": result.get(\"eval_duration\", 0),\n                    \"total_duration\": result.get(\"total_duration\", 0),\n                },\n            }\n\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Error making request to Ollama API: {str(e)}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Error processing input with Ollama: {str(e)}\")\n            raise\n</code></pre>"},{"location":"sources/agents/ollama/#Docs2KG.agents.ollama.OllamaAgent.__init__","title":"<code>__init__(name)</code>","text":"<p>Initialize OllamaAgent with model name and optional API base URL.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the Ollama model to use (e.g., 'llama2', 'mistral')</p> required Source code in <code>Docs2KG/agents/ollama.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"\n    Initialize OllamaAgent with model name and optional API base URL.\n\n    Args:\n        name: Name of the Ollama model to use (e.g., 'llama2', 'mistral')\n    \"\"\"\n    super().__init__(name)\n    self.session = self._init_session()\n</code></pre>"},{"location":"sources/agents/ollama/#Docs2KG.agents.ollama.OllamaAgent.process","title":"<code>process(input_data, reset_session=False)</code>","text":"<p>Process input using the Ollama API.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The input to be processed by the model</p> required <code>reset_session</code> <code>bool</code> <p>Whether to reset the requests session before making the API call</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>Dict containing the model response and metadata</p> Source code in <code>Docs2KG/agents/ollama.py</code> <pre><code>def process(self, input_data: Any, reset_session: bool = False) -&gt; Any:\n    \"\"\"\n    Process input using the Ollama API.\n\n    Args:\n        input_data: The input to be processed by the model\n        reset_session: Whether to reset the requests session before making the API call\n\n    Returns:\n        Dict containing the model response and metadata\n    \"\"\"\n    logger.info(f\"Using Ollama model: {self.name}\")\n\n    try:\n        # Prepare the request\n        url = f\"{self.api_base}/api/generate\"\n\n        payload = {\n            \"model\": self.name,\n            \"prompt\": str(input_data),\n            \"temperature\": PROJECT_CONFIG.ollama.temperature,\n            \"stream\": False,\n            \"format\": PROJECT_CONFIG.ollama.format,\n        }\n\n        # Make the API call\n        if reset_session:\n            self.reset_session()\n        response = self.session.post(\n            url, json=payload, timeout=PROJECT_CONFIG.ollama.timeout\n        )\n        response.raise_for_status()\n\n        result = response.json()\n\n        return {\n            \"model\": self.name,\n            \"input\": input_data,\n            \"status\": \"processed\",\n            \"response\": result.get(\"response\", \"\"),\n            \"usage\": {\n                \"eval_count\": result.get(\"eval_count\", 0),\n                \"eval_duration\": result.get(\"eval_duration\", 0),\n                \"total_duration\": result.get(\"total_duration\", 0),\n            },\n        }\n\n    except requests.exceptions.RequestException as e:\n        logger.error(f\"Error making request to Ollama API: {str(e)}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Error processing input with Ollama: {str(e)}\")\n        raise\n</code></pre>"},{"location":"sources/agents/ollama/#Docs2KG.agents.ollama.OllamaAgent.reset_session","title":"<code>reset_session()</code>","text":"<p>Reset the requests session</p> Source code in <code>Docs2KG/agents/ollama.py</code> <pre><code>def reset_session(self):\n    \"\"\"Reset the requests session\"\"\"\n    self.session.close()\n    self.session = self._init_session()\n</code></pre>"},{"location":"sources/agents/quantization/","title":"Quantization","text":""},{"location":"sources/agents/quantization/#Docs2KG.agents.quantization.QuantizationAgent","title":"<code>QuantizationAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>Docs2KG/agents/quantization.py</code> <pre><code>class QuantizationAgent(BaseAgent):\n    def __init__(self, name: str):\n        \"\"\"\n        Initialize QuantizationAgent with model name.\n\n        Args:\n            name: Path to the quantized model file (e.g., 'models/llama-7b-q4.gguf')\n        \"\"\"\n        super().__init__(name)\n        self.client = self._init_llama_client()\n\n    def _init_llama_client(self) -&gt; Llama:\n        \"\"\"\n        Initialize llama.cpp client with the quantized model.\n        \"\"\"\n        try:\n            # Initialize client with config\n            client = Llama(\n                model_path=self.name or PROJECT_CONFIG.llamacpp.model_path,\n                n_ctx=PROJECT_CONFIG.llamacpp.context_length,\n                n_threads=PROJECT_CONFIG.llamacpp.num_threads,\n                n_gpu_layers=PROJECT_CONFIG.llamacpp.gpu_layers,\n            )\n\n            logger.info(\n                f\"Successfully initialized llamacpp.cpp client for model {self.name}\"\n            )\n            return client\n\n        except Exception as e:\n            logger.error(f\"Failed to initialize llamacpp.cpp client: {str(e)}\")\n            raise\n\n    def process(self, input_data: Any) -&gt; Any:\n        \"\"\"\n        Process input using the llamacpp.cpp client.\n\n        Args:\n            input_data: The input to be processed by the model\n\n        Returns:\n            Dict containing the model response and metadata\n        \"\"\"\n        logger.info(f\"Processing input with llamacpp.cpp: {input_data}\")\n\n        try:\n            # Create completion with proper error handling\n            response = self.client.create_completion(\n                prompt=str(input_data),\n                max_tokens=PROJECT_CONFIG.llamacpp.max_tokens,\n                temperature=PROJECT_CONFIG.llamacpp.temperature,\n                top_p=PROJECT_CONFIG.llamacpp.top_p,\n                stop=PROJECT_CONFIG.llamacpp.stop_tokens,\n                echo=False,  # Don't include prompt in the response\n            )\n\n            # Extract completion tokens used from response metadata\n            completion_tokens = len(response[\"choices\"][0][\"text\"].split())\n            prompt_tokens = len(str(input_data).split())\n\n            return {\n                \"model\": self.name,\n                \"input\": input_data,\n                \"status\": \"processed\",\n                \"response\": response[\"choices\"][0][\"text\"],\n                \"usage\": {\n                    \"prompt_tokens\": prompt_tokens,\n                    \"completion_tokens\": completion_tokens,\n                    \"total_tokens\": prompt_tokens + completion_tokens,\n                },\n            }\n\n        except Exception as e:\n            logger.error(f\"Error processing input with llamacpp.cpp: {str(e)}\")\n            raise\n</code></pre>"},{"location":"sources/agents/quantization/#Docs2KG.agents.quantization.QuantizationAgent.__init__","title":"<code>__init__(name)</code>","text":"<p>Initialize QuantizationAgent with model name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Path to the quantized model file (e.g., 'models/llama-7b-q4.gguf')</p> required Source code in <code>Docs2KG/agents/quantization.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"\n    Initialize QuantizationAgent with model name.\n\n    Args:\n        name: Path to the quantized model file (e.g., 'models/llama-7b-q4.gguf')\n    \"\"\"\n    super().__init__(name)\n    self.client = self._init_llama_client()\n</code></pre>"},{"location":"sources/agents/quantization/#Docs2KG.agents.quantization.QuantizationAgent.process","title":"<code>process(input_data)</code>","text":"<p>Process input using the llamacpp.cpp client.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The input to be processed by the model</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Dict containing the model response and metadata</p> Source code in <code>Docs2KG/agents/quantization.py</code> <pre><code>def process(self, input_data: Any) -&gt; Any:\n    \"\"\"\n    Process input using the llamacpp.cpp client.\n\n    Args:\n        input_data: The input to be processed by the model\n\n    Returns:\n        Dict containing the model response and metadata\n    \"\"\"\n    logger.info(f\"Processing input with llamacpp.cpp: {input_data}\")\n\n    try:\n        # Create completion with proper error handling\n        response = self.client.create_completion(\n            prompt=str(input_data),\n            max_tokens=PROJECT_CONFIG.llamacpp.max_tokens,\n            temperature=PROJECT_CONFIG.llamacpp.temperature,\n            top_p=PROJECT_CONFIG.llamacpp.top_p,\n            stop=PROJECT_CONFIG.llamacpp.stop_tokens,\n            echo=False,  # Don't include prompt in the response\n        )\n\n        # Extract completion tokens used from response metadata\n        completion_tokens = len(response[\"choices\"][0][\"text\"].split())\n        prompt_tokens = len(str(input_data).split())\n\n        return {\n            \"model\": self.name,\n            \"input\": input_data,\n            \"status\": \"processed\",\n            \"response\": response[\"choices\"][0][\"text\"],\n            \"usage\": {\n                \"prompt_tokens\": prompt_tokens,\n                \"completion_tokens\": completion_tokens,\n                \"total_tokens\": prompt_tokens + completion_tokens,\n            },\n        }\n\n    except Exception as e:\n        logger.error(f\"Error processing input with llamacpp.cpp: {str(e)}\")\n        raise\n</code></pre>"},{"location":"sources/agents/func/ner_llm_judge/","title":"Ner llm judge","text":""},{"location":"sources/digitization/base/","title":"Base","text":"<p>Base class for digitization methods that handles conversion of various input formats into standardized digital representations.</p>"},{"location":"sources/digitization/base/#Docs2KG.digitization.base.DigitizationBase","title":"<code>DigitizationBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for digitization agents that defines the common interface and functionality for all digitization implementations.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique identifier for the digitization agent</p> <code>supported_formats</code> <code>List[str]</code> <p>List of input formats this agent can process</p> <p>The output will be export to - markdown - json for table - json for images and files</p> Source code in <code>Docs2KG/digitization/base.py</code> <pre><code>class DigitizationBase(ABC):\n    \"\"\"\n    Abstract base class for digitization agents that defines the common interface\n    and functionality for all digitization implementations.\n\n    Attributes:\n        name (str): Unique identifier for the digitization agent\n        supported_formats (List[str]): List of input formats this agent can process\n\n    The output will be export to\n    - markdown\n    - json for table\n    - json for images and files\n    \"\"\"\n\n    def __init__(\n        self,\n        file_path: Path,\n        supported_formats: Optional[List[str]] = None,\n    ):\n        self.file_path = file_path\n        self.filename = file_path.stem\n        self.name = self.__class__.__name__\n        self.supported_formats = supported_formats or []\n\n    @property\n    def output_dir(self) -&gt; Path:\n        \"\"\"\n        Get the output directory for the digitization agent.\n\n        Returns:\n            str: Output directory path\n        \"\"\"\n        output_dir_path = PROJECT_CONFIG.data.output_dir\n        # based on the filename, we will create a folder\n        output_dir = Path(output_dir_path) / self.filename / self.name\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        # create a sub folder for images\n        images_dir = output_dir / \"images\"\n        images_dir.mkdir(parents=True, exist_ok=True)\n        return output_dir\n\n    @abstractmethod\n    def process(self, input_data: Any) -&gt; Union[Dict, Any]:\n        \"\"\"\n        Process the input data and return digitized output.\n\n        Args:\n            input_data: The data to be digitized\n\n        Returns:\n            Digitized representation of the input data\n\n        Raises:\n            NotImplementedError: If the child class doesn't implement this method\n            ValueError: If input format is not supported\n        \"\"\"\n        raise NotImplementedError(\"Each digitization agent must implement process()\")\n\n    def export_content_to_markdown_file(self, text: str) -&gt; Path:\n        with open(self.output_dir / f\"{self.filename}.md\", \"w\") as f:\n            f.write(text)\n\n        return self.output_dir / f\"{self.filename}.md\"\n\n    def export_table_to_json_file(self, data: Dict) -&gt; Path:\n        with open(self.output_dir / f\"{self.filename}_table.json\", \"w\") as f:\n            f.write(json.dumps(data, indent=4))\n\n        return self.output_dir / f\"{self.filename}_table.json\"\n\n    def export_images_to_json_file(self, data: Dict) -&gt; Path:\n        with open(self.output_dir / f\"{self.filename}_images.json\", \"w\") as f:\n            f.write(json.dumps(data, indent=4))\n\n        return self.output_dir / f\"{self.filename}_images.json\"\n\n    def validate_input(self, input_data: Any) -&gt; bool:\n        \"\"\"\n        Validate if the input data format is supported by this agent.\n\n        Args:\n            input_data: The data to validate\n\n        Returns:\n            bool: True if input format is supported, False otherwise\n        \"\"\"\n        return True  # Base implementation accepts all formats\n\n    def get_agent_info(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get information about the digitization agent.\n\n        Returns:\n            Dict containing agent metadata and configuration\n        \"\"\"\n        return {\n            \"name\": self.__class__.__name__,\n            \"supported_formats\": self.supported_formats,\n        }\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(\" f\"supported_formats={self.supported_formats})\"\n        )\n\n    def __str__(self) -&gt; str:\n        return f\"{self.name} Digitization Agent\"\n</code></pre>"},{"location":"sources/digitization/base/#Docs2KG.digitization.base.DigitizationBase.output_dir","title":"<code>output_dir</code>  <code>property</code>","text":"<p>Get the output directory for the digitization agent.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Path</code> <p>Output directory path</p>"},{"location":"sources/digitization/base/#Docs2KG.digitization.base.DigitizationBase.get_agent_info","title":"<code>get_agent_info()</code>","text":"<p>Get information about the digitization agent.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing agent metadata and configuration</p> Source code in <code>Docs2KG/digitization/base.py</code> <pre><code>def get_agent_info(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get information about the digitization agent.\n\n    Returns:\n        Dict containing agent metadata and configuration\n    \"\"\"\n    return {\n        \"name\": self.__class__.__name__,\n        \"supported_formats\": self.supported_formats,\n    }\n</code></pre>"},{"location":"sources/digitization/base/#Docs2KG.digitization.base.DigitizationBase.process","title":"<code>process(input_data)</code>  <code>abstractmethod</code>","text":"<p>Process the input data and return digitized output.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The data to be digitized</p> required <p>Returns:</p> Type Description <code>Union[Dict, Any]</code> <p>Digitized representation of the input data</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the child class doesn't implement this method</p> <code>ValueError</code> <p>If input format is not supported</p> Source code in <code>Docs2KG/digitization/base.py</code> <pre><code>@abstractmethod\ndef process(self, input_data: Any) -&gt; Union[Dict, Any]:\n    \"\"\"\n    Process the input data and return digitized output.\n\n    Args:\n        input_data: The data to be digitized\n\n    Returns:\n        Digitized representation of the input data\n\n    Raises:\n        NotImplementedError: If the child class doesn't implement this method\n        ValueError: If input format is not supported\n    \"\"\"\n    raise NotImplementedError(\"Each digitization agent must implement process()\")\n</code></pre>"},{"location":"sources/digitization/base/#Docs2KG.digitization.base.DigitizationBase.validate_input","title":"<code>validate_input(input_data)</code>","text":"<p>Validate if the input data format is supported by this agent.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The data to validate</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if input format is supported, False otherwise</p> Source code in <code>Docs2KG/digitization/base.py</code> <pre><code>def validate_input(self, input_data: Any) -&gt; bool:\n    \"\"\"\n    Validate if the input data format is supported by this agent.\n\n    Args:\n        input_data: The data to validate\n\n    Returns:\n        bool: True if input format is supported, False otherwise\n    \"\"\"\n    return True  # Base implementation accepts all formats\n</code></pre>"},{"location":"sources/digitization/image/pdf_docling/","title":"Pdf docling","text":""},{"location":"sources/digitization/image/pdf_docling/#Docs2KG.digitization.image.pdf_docling.PDFDocling","title":"<code>PDFDocling</code>","text":"<p>               Bases: <code>DigitizationBase</code></p> <p>Enhanced PDFDocling class with separate exports for markdown, images, and tables.</p> Source code in <code>Docs2KG/digitization/image/pdf_docling.py</code> <pre><code>class PDFDocling(DigitizationBase):\n    \"\"\"\n    Enhanced PDFDocling class with separate exports for markdown, images, and tables.\n    \"\"\"\n\n    def __init__(self, file_path: Path):\n        super().__init__(file_path=file_path, supported_formats=[InputFormat.PDF])\n        pipeline_options = PdfPipelineOptions()\n        pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n        pipeline_options.generate_page_images = True\n        pipeline_options.generate_picture_images = True\n\n        self.converter = DocumentConverter(\n            format_options={\n                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n            }\n        )\n\n    @staticmethod\n    def validate_input(input_data: Union[str, Path]) -&gt; bool:\n        try:\n            if isinstance(input_data, str) and input_data.startswith(\n                (\"http://\", \"https://\")\n            ):\n                return input_data.lower().endswith(\".pdf\")\n            path = Path(input_data)\n            return path.exists() and path.suffix.lower() == \".pdf\"\n        except Exception as e:\n            logger.exception(f\"Error validating input: {str(e)}\")\n            return False\n\n    def export_markdown(self, document) -&gt; Path:\n        \"\"\"Export document content to markdown file.\"\"\"\n        markdown_path = self.output_dir / f\"{self.filename}.md\"\n        document.save_as_markdown(\n            markdown_path,\n            image_mode=ImageRefMode.REFERENCED,\n            artifacts_dir=self.output_dir / \"images\",\n        )\n        return markdown_path\n\n    def process(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Process PDF document and generate all outputs.\n        \"\"\"\n        if not self.validate_input(self.file_path):\n            raise ValueError(\n                f\"Invalid input: {self.file_path}. Expected valid PDF file path or URL\"\n            )\n\n        try:\n            # Convert the document\n            result = self.converter.convert(str(self.file_path))\n\n            # Generate all outputs\n            markdown_path = self.export_markdown(result.document)\n            return markdown_path\n\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"PDF source not found: {self.file_path}\")\n        except Exception as e:\n            raise Exception(f\"Error processing PDF: {str(e)}\")\n\n    def __repr__(self) -&gt; str:\n        return f\"PDFDocling(file_path='{self.file_path}')\"\n</code></pre>"},{"location":"sources/digitization/image/pdf_docling/#Docs2KG.digitization.image.pdf_docling.PDFDocling.export_markdown","title":"<code>export_markdown(document)</code>","text":"<p>Export document content to markdown file.</p> Source code in <code>Docs2KG/digitization/image/pdf_docling.py</code> <pre><code>def export_markdown(self, document) -&gt; Path:\n    \"\"\"Export document content to markdown file.\"\"\"\n    markdown_path = self.output_dir / f\"{self.filename}.md\"\n    document.save_as_markdown(\n        markdown_path,\n        image_mode=ImageRefMode.REFERENCED,\n        artifacts_dir=self.output_dir / \"images\",\n    )\n    return markdown_path\n</code></pre>"},{"location":"sources/digitization/image/pdf_docling/#Docs2KG.digitization.image.pdf_docling.PDFDocling.process","title":"<code>process()</code>","text":"<p>Process PDF document and generate all outputs.</p> Source code in <code>Docs2KG/digitization/image/pdf_docling.py</code> <pre><code>def process(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process PDF document and generate all outputs.\n    \"\"\"\n    if not self.validate_input(self.file_path):\n        raise ValueError(\n            f\"Invalid input: {self.file_path}. Expected valid PDF file path or URL\"\n        )\n\n    try:\n        # Convert the document\n        result = self.converter.convert(str(self.file_path))\n\n        # Generate all outputs\n        markdown_path = self.export_markdown(result.document)\n        return markdown_path\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"PDF source not found: {self.file_path}\")\n    except Exception as e:\n        raise Exception(f\"Error processing PDF: {str(e)}\")\n</code></pre>"},{"location":"sources/digitization/native/ebook/","title":"Ebook","text":""},{"location":"sources/digitization/native/ebook/#Docs2KG.digitization.native.ebook.Chapter","title":"<code>Chapter</code>  <code>dataclass</code>","text":"<p>Data class for storing chapter information</p> Source code in <code>Docs2KG/digitization/native/ebook.py</code> <pre><code>@dataclass\nclass Chapter:\n    \"\"\"Data class for storing chapter information\"\"\"\n\n    title: str\n    content: str\n    order: int\n    images: List[Dict[str, str]]\n</code></pre>"},{"location":"sources/digitization/native/ebook/#Docs2KG.digitization.native.ebook.EPUBDigitization","title":"<code>EPUBDigitization</code>","text":"<p>               Bases: <code>DigitizationBase</code></p> <p>EPUB digitization agent that converts EPUB files to markdown, extracts images, and generates structured data.</p> <p>Inherits from DigitizationBase and implements EPUB-specific processing logic.</p> Source code in <code>Docs2KG/digitization/native/ebook.py</code> <pre><code>class EPUBDigitization(DigitizationBase):\n    \"\"\"\n    EPUB digitization agent that converts EPUB files to markdown, extracts images,\n    and generates structured data.\n\n    Inherits from DigitizationBase and implements EPUB-specific processing logic.\n    \"\"\"\n\n    def __init__(self, file_path: Path):\n        super().__init__(file_path=file_path, supported_formats=[\"epub\"])\n        self.book = None\n        self.chapters: List[Chapter] = []\n        self.metadata: Dict[str, Any] = {}\n        self.image_counter = 0\n\n        # Configure HTML to Text converter\n        self.text_maker = html2text.HTML2Text()\n        self.text_maker.ignore_links = False\n        self.text_maker.ignore_images = False\n        self.text_maker.body_width = 0\n\n    def validate_input(self, input_data: Union[str, Path]) -&gt; bool:\n        \"\"\"\n        Validate if the input is a valid EPUB file.\n\n        Args:\n            input_data: File path to validate\n\n        Returns:\n            bool: True if input is valid EPUB, False otherwise\n        \"\"\"\n        try:\n            path = Path(input_data)\n            if not path.exists():\n                logger.error(f\"File not found: {path}\")\n                return False\n\n            if path.suffix.lower() != \".epub\":\n                logger.error(f\"Invalid file format: {path.suffix}\")\n                return False\n\n            # Try to load the EPUB file\n            epub.read_epub(str(path))\n            return True\n\n        except Exception as e:\n            logger.error(f\"Error validating EPUB: {str(e)}\")\n            return False\n\n    def extract_images_from_chapter(\n        self, chapter_content: str, chapter_id: str\n    ) -&gt; List[Dict[str, str]]:\n        \"\"\"\n        Extract and save images from chapter content.\n\n        Args:\n            chapter_content: HTML content of the chapter\n            chapter_id: Identifier for the chapter\n\n        Returns:\n            List of dictionaries containing image information\n        \"\"\"\n        images = []\n        soup = BeautifulSoup(chapter_content, \"html.parser\")\n\n        for img in soup.find_all(\"img\"):\n            try:\n                img_src = img.get(\"src\", \"\")\n                if not img_src:\n                    continue\n\n                self.image_counter += 1\n                img_filename = f\"image_{chapter_id}_{self.image_counter}.jpg\"\n                img_path = self.output_dir / \"images\" / img_filename\n\n                for item in self.book.get_items_of_type(ebooklib.ITEM_IMAGE):\n                    if item.file_name.endswith(img_src.split(\"/\")[-1]):\n                        with open(img_path, \"wb\") as f:\n                            f.write(item.content)\n\n                        images.append(\n                            {\n                                \"original_src\": img_src,\n                                \"saved_path\": str(\n                                    img_path.relative_to(self.output_dir)\n                                ),\n                                \"alt_text\": img.get(\"alt\", \"\"),\n                                \"chapter_id\": chapter_id,\n                            }\n                        )\n\n                        img[\"src\"] = str(img_path.relative_to(self.output_dir))\n\n            except Exception as e:\n                logger.error(f\"Error processing image: {str(e)}\")\n\n        return images\n\n    def process_chapter(self, chapter_item, order: int) -&gt; Optional[Chapter]:\n        \"\"\"\n        Process a single chapter from the EPUB.\n\n        Args:\n            chapter_item: EpubItem containing chapter content\n            order: Chapter order number\n\n        Returns:\n            Chapter object or None if processing fails\n        \"\"\"\n        try:\n            content = chapter_item.get_content().decode(\"utf-8\")\n            soup = BeautifulSoup(content, \"html.parser\")\n\n            title = soup.find(\"title\")\n            title = title.text if title else f\"Chapter {order}\"\n\n            images = self.extract_images_from_chapter(content, f\"ch{order}\")\n            markdown_content = self.text_maker.handle(str(soup))\n\n            return Chapter(\n                title=title, content=markdown_content, order=order, images=images\n            )\n\n        except Exception as e:\n            logger.error(f\"Error processing chapter: {str(e)}\")\n            return None\n\n    def extract_metadata(self):\n        \"\"\"Extract metadata from the EPUB file.\"\"\"\n        try:\n            self.metadata = {\n                \"title\": self.book.get_metadata(\"DC\", \"title\"),\n                \"creator\": self.book.get_metadata(\"DC\", \"creator\"),\n                \"language\": self.book.get_metadata(\"DC\", \"language\"),\n                \"publisher\": self.book.get_metadata(\"DC\", \"publisher\"),\n                \"identifier\": self.book.get_metadata(\"DC\", \"identifier\"),\n                \"date\": self.book.get_metadata(\"DC\", \"date\"),\n            }\n\n            # Clean up metadata values\n            for key, value in self.metadata.items():\n                if isinstance(value, list) and value:\n                    self.metadata[key] = value[0][0]\n                elif not value:\n                    self.metadata[key] = None\n\n        except Exception as e:\n            logger.error(f\"Error extracting metadata: {str(e)}\")\n\n    def process(self, input_data: Optional[Any] = None) -&gt; Dict[str, Any]:\n        \"\"\"\n        Process the EPUB file and generate all outputs.\n\n        Args:\n            input_data: Optional additional input data (not used in this implementation)\n\n        Returns:\n            Dictionary containing paths to generated outputs\n        \"\"\"\n        if not self.validate_input(self.file_path):\n            raise ValueError(f\"Invalid EPUB file: {self.file_path}\")\n\n        try:\n            # Read the EPUB file\n            self.book = epub.read_epub(str(self.file_path))\n\n            # Extract metadata\n            self.extract_metadata()\n\n            # Process chapters\n            all_images = []\n            markdown_content = []\n\n            # Add metadata section\n            markdown_content.append(\"---\")\n            for key, value in self.metadata.items():\n                if value:\n                    markdown_content.append(f\"{key}: {value}\")\n            markdown_content.append(\"---\\n\")\n\n            # Process chapters and collect content\n            for idx, item in enumerate(\n                self.book.get_items_of_type(ebooklib.ITEM_DOCUMENT)\n            ):\n                chapter = self.process_chapter(item, idx)\n                if chapter:\n                    self.chapters.append(chapter)\n                    markdown_content.extend(\n                        [f\"# {chapter.title}\\n\", chapter.content, \"---\\n\"]\n                    )\n                    all_images.extend(chapter.images)\n\n            # Export markdown content\n            markdown_path = self.export_content_to_markdown_file(\n                \"\\n\".join(markdown_content)\n            )\n\n            # Export images data\n            images_data = {\"total_images\": len(all_images), \"images\": all_images}\n            images_json_path = self.export_images_to_json_file(images_data)\n\n            # Export basic structure data as table\n            structure_data = {\n                \"metadata\": self.metadata,\n                \"chapters\": [\n                    {\n                        \"title\": ch.title,\n                        \"order\": ch.order,\n                        \"image_count\": len(ch.images),\n                    }\n                    for ch in self.chapters\n                ],\n            }\n            table_json_path = self.export_table_to_json_file(structure_data)\n\n            return {\n                \"markdown_path\": markdown_path,\n                \"images_json_path\": images_json_path,\n                \"table_json_path\": table_json_path,\n                \"total_chapters\": len(self.chapters),\n                \"total_images\": len(all_images),\n            }\n\n        except Exception as e:\n            logger.error(f\"Error processing EPUB: {str(e)}\")\n            raise\n</code></pre>"},{"location":"sources/digitization/native/ebook/#Docs2KG.digitization.native.ebook.EPUBDigitization.extract_images_from_chapter","title":"<code>extract_images_from_chapter(chapter_content, chapter_id)</code>","text":"<p>Extract and save images from chapter content.</p> <p>Parameters:</p> Name Type Description Default <code>chapter_content</code> <code>str</code> <p>HTML content of the chapter</p> required <code>chapter_id</code> <code>str</code> <p>Identifier for the chapter</p> required <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List of dictionaries containing image information</p> Source code in <code>Docs2KG/digitization/native/ebook.py</code> <pre><code>def extract_images_from_chapter(\n    self, chapter_content: str, chapter_id: str\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Extract and save images from chapter content.\n\n    Args:\n        chapter_content: HTML content of the chapter\n        chapter_id: Identifier for the chapter\n\n    Returns:\n        List of dictionaries containing image information\n    \"\"\"\n    images = []\n    soup = BeautifulSoup(chapter_content, \"html.parser\")\n\n    for img in soup.find_all(\"img\"):\n        try:\n            img_src = img.get(\"src\", \"\")\n            if not img_src:\n                continue\n\n            self.image_counter += 1\n            img_filename = f\"image_{chapter_id}_{self.image_counter}.jpg\"\n            img_path = self.output_dir / \"images\" / img_filename\n\n            for item in self.book.get_items_of_type(ebooklib.ITEM_IMAGE):\n                if item.file_name.endswith(img_src.split(\"/\")[-1]):\n                    with open(img_path, \"wb\") as f:\n                        f.write(item.content)\n\n                    images.append(\n                        {\n                            \"original_src\": img_src,\n                            \"saved_path\": str(\n                                img_path.relative_to(self.output_dir)\n                            ),\n                            \"alt_text\": img.get(\"alt\", \"\"),\n                            \"chapter_id\": chapter_id,\n                        }\n                    )\n\n                    img[\"src\"] = str(img_path.relative_to(self.output_dir))\n\n        except Exception as e:\n            logger.error(f\"Error processing image: {str(e)}\")\n\n    return images\n</code></pre>"},{"location":"sources/digitization/native/ebook/#Docs2KG.digitization.native.ebook.EPUBDigitization.extract_metadata","title":"<code>extract_metadata()</code>","text":"<p>Extract metadata from the EPUB file.</p> Source code in <code>Docs2KG/digitization/native/ebook.py</code> <pre><code>def extract_metadata(self):\n    \"\"\"Extract metadata from the EPUB file.\"\"\"\n    try:\n        self.metadata = {\n            \"title\": self.book.get_metadata(\"DC\", \"title\"),\n            \"creator\": self.book.get_metadata(\"DC\", \"creator\"),\n            \"language\": self.book.get_metadata(\"DC\", \"language\"),\n            \"publisher\": self.book.get_metadata(\"DC\", \"publisher\"),\n            \"identifier\": self.book.get_metadata(\"DC\", \"identifier\"),\n            \"date\": self.book.get_metadata(\"DC\", \"date\"),\n        }\n\n        # Clean up metadata values\n        for key, value in self.metadata.items():\n            if isinstance(value, list) and value:\n                self.metadata[key] = value[0][0]\n            elif not value:\n                self.metadata[key] = None\n\n    except Exception as e:\n        logger.error(f\"Error extracting metadata: {str(e)}\")\n</code></pre>"},{"location":"sources/digitization/native/ebook/#Docs2KG.digitization.native.ebook.EPUBDigitization.process","title":"<code>process(input_data=None)</code>","text":"<p>Process the EPUB file and generate all outputs.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Optional[Any]</code> <p>Optional additional input data (not used in this implementation)</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing paths to generated outputs</p> Source code in <code>Docs2KG/digitization/native/ebook.py</code> <pre><code>def process(self, input_data: Optional[Any] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process the EPUB file and generate all outputs.\n\n    Args:\n        input_data: Optional additional input data (not used in this implementation)\n\n    Returns:\n        Dictionary containing paths to generated outputs\n    \"\"\"\n    if not self.validate_input(self.file_path):\n        raise ValueError(f\"Invalid EPUB file: {self.file_path}\")\n\n    try:\n        # Read the EPUB file\n        self.book = epub.read_epub(str(self.file_path))\n\n        # Extract metadata\n        self.extract_metadata()\n\n        # Process chapters\n        all_images = []\n        markdown_content = []\n\n        # Add metadata section\n        markdown_content.append(\"---\")\n        for key, value in self.metadata.items():\n            if value:\n                markdown_content.append(f\"{key}: {value}\")\n        markdown_content.append(\"---\\n\")\n\n        # Process chapters and collect content\n        for idx, item in enumerate(\n            self.book.get_items_of_type(ebooklib.ITEM_DOCUMENT)\n        ):\n            chapter = self.process_chapter(item, idx)\n            if chapter:\n                self.chapters.append(chapter)\n                markdown_content.extend(\n                    [f\"# {chapter.title}\\n\", chapter.content, \"---\\n\"]\n                )\n                all_images.extend(chapter.images)\n\n        # Export markdown content\n        markdown_path = self.export_content_to_markdown_file(\n            \"\\n\".join(markdown_content)\n        )\n\n        # Export images data\n        images_data = {\"total_images\": len(all_images), \"images\": all_images}\n        images_json_path = self.export_images_to_json_file(images_data)\n\n        # Export basic structure data as table\n        structure_data = {\n            \"metadata\": self.metadata,\n            \"chapters\": [\n                {\n                    \"title\": ch.title,\n                    \"order\": ch.order,\n                    \"image_count\": len(ch.images),\n                }\n                for ch in self.chapters\n            ],\n        }\n        table_json_path = self.export_table_to_json_file(structure_data)\n\n        return {\n            \"markdown_path\": markdown_path,\n            \"images_json_path\": images_json_path,\n            \"table_json_path\": table_json_path,\n            \"total_chapters\": len(self.chapters),\n            \"total_images\": len(all_images),\n        }\n\n    except Exception as e:\n        logger.error(f\"Error processing EPUB: {str(e)}\")\n        raise\n</code></pre>"},{"location":"sources/digitization/native/ebook/#Docs2KG.digitization.native.ebook.EPUBDigitization.process_chapter","title":"<code>process_chapter(chapter_item, order)</code>","text":"<p>Process a single chapter from the EPUB.</p> <p>Parameters:</p> Name Type Description Default <code>chapter_item</code> <p>EpubItem containing chapter content</p> required <code>order</code> <code>int</code> <p>Chapter order number</p> required <p>Returns:</p> Type Description <code>Optional[Chapter]</code> <p>Chapter object or None if processing fails</p> Source code in <code>Docs2KG/digitization/native/ebook.py</code> <pre><code>def process_chapter(self, chapter_item, order: int) -&gt; Optional[Chapter]:\n    \"\"\"\n    Process a single chapter from the EPUB.\n\n    Args:\n        chapter_item: EpubItem containing chapter content\n        order: Chapter order number\n\n    Returns:\n        Chapter object or None if processing fails\n    \"\"\"\n    try:\n        content = chapter_item.get_content().decode(\"utf-8\")\n        soup = BeautifulSoup(content, \"html.parser\")\n\n        title = soup.find(\"title\")\n        title = title.text if title else f\"Chapter {order}\"\n\n        images = self.extract_images_from_chapter(content, f\"ch{order}\")\n        markdown_content = self.text_maker.handle(str(soup))\n\n        return Chapter(\n            title=title, content=markdown_content, order=order, images=images\n        )\n\n    except Exception as e:\n        logger.error(f\"Error processing chapter: {str(e)}\")\n        return None\n</code></pre>"},{"location":"sources/digitization/native/ebook/#Docs2KG.digitization.native.ebook.EPUBDigitization.validate_input","title":"<code>validate_input(input_data)</code>","text":"<p>Validate if the input is a valid EPUB file.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[str, Path]</code> <p>File path to validate</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if input is valid EPUB, False otherwise</p> Source code in <code>Docs2KG/digitization/native/ebook.py</code> <pre><code>def validate_input(self, input_data: Union[str, Path]) -&gt; bool:\n    \"\"\"\n    Validate if the input is a valid EPUB file.\n\n    Args:\n        input_data: File path to validate\n\n    Returns:\n        bool: True if input is valid EPUB, False otherwise\n    \"\"\"\n    try:\n        path = Path(input_data)\n        if not path.exists():\n            logger.error(f\"File not found: {path}\")\n            return False\n\n        if path.suffix.lower() != \".epub\":\n            logger.error(f\"Invalid file format: {path.suffix}\")\n            return False\n\n        # Try to load the EPUB file\n        epub.read_epub(str(path))\n        return True\n\n    except Exception as e:\n        logger.error(f\"Error validating EPUB: {str(e)}\")\n        return False\n</code></pre>"},{"location":"sources/digitization/native/html_parser/","title":"Html parser","text":""},{"location":"sources/digitization/native/html_parser/#Docs2KG.digitization.native.html_parser.HTMLDocling","title":"<code>HTMLDocling</code>","text":"<p>               Bases: <code>DigitizationBase</code></p> <p>HTMLDocling class for processing HTML content from files or URLs to markdown.</p> Source code in <code>Docs2KG/digitization/native/html_parser.py</code> <pre><code>class HTMLDocling(DigitizationBase):\n    \"\"\"\n    HTMLDocling class for processing HTML content from files or URLs to markdown.\n    \"\"\"\n\n    def __init__(self, file_path: Union[str, Path]):\n        self.is_url = isinstance(file_path, str) and self._is_valid_url(file_path)\n\n        if self.is_url:\n            # Create a filename from the URL\n            url_path = urlparse(file_path).path\n            url_filename = (\n                unquote(Path(url_path).name)\n                if url_path and Path(url_path).name\n                else urlparse(file_path).netloc\n            )\n            self.html_filename = (\n                f\"{url_filename}.html\"\n                if not url_filename.endswith(\".html\")\n                else url_filename\n            )\n\n            # Download and save the HTML content\n            self.html_path = PROJECT_CONFIG.data.input_dir / self.html_filename\n            self._download_and_save_html(file_path)\n\n            # Use the saved file path as the source\n            file_path = self.html_path\n\n        super().__init__(\n            file_path=file_path,\n            supported_formats=[\"html\", \"htm\"],\n        )\n        self.source = file_path\n\n    def _download_and_save_html(self, url: str) -&gt; None:\n        \"\"\"\n        Download HTML content from URL and save to file.\n\n        Args:\n            url: URL to download from\n        \"\"\"\n        try:\n            headers = {\n                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n            }\n            response = requests.get(url, headers=headers, timeout=30)\n            response.raise_for_status()\n\n            # Ensure input directory exists\n            PROJECT_CONFIG.data.input_dir.mkdir(parents=True, exist_ok=True)\n\n            # Save the HTML content\n            self.html_path.write_text(response.text, encoding=\"utf-8\")\n            logger.info(f\"Saved HTML content to {self.html_path}\")\n\n        except Exception as e:\n            raise Exception(f\"Error downloading URL {url}: {str(e)}\")\n\n    @staticmethod\n    def _is_valid_url(url: str) -&gt; bool:\n        try:\n            result = urlparse(url)\n            return all([result.scheme, result.netloc])\n        except Exception as e:  # noqa\n            logger.exception(f\"Error validating URL: {str(e)}\")\n            return False\n\n    def validate_input(self, input_data: Union[str, Path]) -&gt; bool:\n        try:\n            if isinstance(input_data, str) and self._is_valid_url(input_data):\n                return True\n            path = Path(input_data)\n            return path.exists() and path.suffix.lower() in [\".html\", \".htm\"]\n        except Exception as e:\n            logger.exception(f\"Error validating input: {str(e)}\")\n            return False\n\n    def clean_html(self, html_content: str) -&gt; str:\n        \"\"\"\n        Clean HTML content by removing styles, scripts, and unnecessary elements.\n        \"\"\"\n        # Parse HTML with BeautifulSoup\n        soup = BeautifulSoup(html_content, \"html.parser\")\n\n        # Remove style tags and their contents\n        for style in soup.find_all(\"style\"):\n            style.decompose()\n\n        # Remove script tags and their contents\n        for script in soup.find_all(\"script\"):\n            script.decompose()\n\n        # Remove all style attributes\n        for tag in soup.find_all(True):\n            if \"style\" in tag.attrs:\n                del tag[\"style\"]\n\n        # Remove class and id attributes\n        for tag in soup.find_all(True):\n            if \"class\" in tag.attrs:\n                del tag[\"class\"]\n            if \"id\" in tag.attrs:\n                del tag[\"id\"]\n\n        # Convert back to string\n        cleaned_html = str(soup)\n\n        # Remove any remaining CSS-like content\n        cleaned_html = re.sub(r\"&lt;style[^&gt;]*&gt;[\\s\\S]*?&lt;/style&gt;\", \"\", cleaned_html)\n        cleaned_html = re.sub(r\"/\\*[\\s\\S]*?\\*/\", \"\", cleaned_html)\n        cleaned_html = re.sub(r\"{\\s*[^}]*}\", \"\", cleaned_html)\n\n        return cleaned_html\n\n    def export_markdown(self, content: str) -&gt; Path:\n        markdown_path = self.output_dir / f\"{self.filename}.md\"\n        markdown_path.write_text(content, encoding=\"utf-8\")\n        return markdown_path\n\n    def get_html_content(self) -&gt; str:\n        try:\n            return Path(self.source).read_text(encoding=\"utf-8\")\n        except UnicodeDecodeError:\n            return Path(self.source).read_text(encoding=\"latin-1\")\n\n    def process(self) -&gt; Path:\n        \"\"\"\n        Process HTML document and generate markdown output.\n        \"\"\"\n        if not self.validate_input(self.source):\n            raise ValueError(\n                f\"Invalid input: {self.source}. Expected valid HTML file or URL\"\n            )\n\n        try:\n            # Get HTML content\n            html_content = self.get_html_content()\n\n            # Clean the HTML content\n            cleaned_html = self.clean_html(html_content)\n\n            # Convert cleaned HTML to markdown\n            markdown_content = markdownify(\n                cleaned_html, heading_style=\"ATX\", bullets=\"-\", autolinks=True\n            )\n\n            # Additional cleanup of the markdown content\n            # Remove empty lines between list items\n            markdown_content = re.sub(r\"\\n\\n-\", \"\\n-\", markdown_content)\n            # Remove multiple consecutive empty lines\n            markdown_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_content)\n            # Remove any remaining CSS-like content\n            markdown_content = re.sub(r\"(\\{|\\}|\\[|\\])[^\\n]*\\n\", \"\", markdown_content)\n\n            # Save markdown content to file\n            markdown_path = self.export_markdown(markdown_content)\n            return markdown_path\n\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"HTML file not found: {self.source}\")\n        except Exception as e:\n            raise Exception(f\"Error processing HTML: {str(e)}\")\n\n    def __repr__(self) -&gt; str:\n        source_type = \"URL\" if self.is_url else \"file\"\n        return f\"HTMLDocling({source_type}='{self.source}')\"\n</code></pre>"},{"location":"sources/digitization/native/html_parser/#Docs2KG.digitization.native.html_parser.HTMLDocling.clean_html","title":"<code>clean_html(html_content)</code>","text":"<p>Clean HTML content by removing styles, scripts, and unnecessary elements.</p> Source code in <code>Docs2KG/digitization/native/html_parser.py</code> <pre><code>def clean_html(self, html_content: str) -&gt; str:\n    \"\"\"\n    Clean HTML content by removing styles, scripts, and unnecessary elements.\n    \"\"\"\n    # Parse HTML with BeautifulSoup\n    soup = BeautifulSoup(html_content, \"html.parser\")\n\n    # Remove style tags and their contents\n    for style in soup.find_all(\"style\"):\n        style.decompose()\n\n    # Remove script tags and their contents\n    for script in soup.find_all(\"script\"):\n        script.decompose()\n\n    # Remove all style attributes\n    for tag in soup.find_all(True):\n        if \"style\" in tag.attrs:\n            del tag[\"style\"]\n\n    # Remove class and id attributes\n    for tag in soup.find_all(True):\n        if \"class\" in tag.attrs:\n            del tag[\"class\"]\n        if \"id\" in tag.attrs:\n            del tag[\"id\"]\n\n    # Convert back to string\n    cleaned_html = str(soup)\n\n    # Remove any remaining CSS-like content\n    cleaned_html = re.sub(r\"&lt;style[^&gt;]*&gt;[\\s\\S]*?&lt;/style&gt;\", \"\", cleaned_html)\n    cleaned_html = re.sub(r\"/\\*[\\s\\S]*?\\*/\", \"\", cleaned_html)\n    cleaned_html = re.sub(r\"{\\s*[^}]*}\", \"\", cleaned_html)\n\n    return cleaned_html\n</code></pre>"},{"location":"sources/digitization/native/html_parser/#Docs2KG.digitization.native.html_parser.HTMLDocling.process","title":"<code>process()</code>","text":"<p>Process HTML document and generate markdown output.</p> Source code in <code>Docs2KG/digitization/native/html_parser.py</code> <pre><code>def process(self) -&gt; Path:\n    \"\"\"\n    Process HTML document and generate markdown output.\n    \"\"\"\n    if not self.validate_input(self.source):\n        raise ValueError(\n            f\"Invalid input: {self.source}. Expected valid HTML file or URL\"\n        )\n\n    try:\n        # Get HTML content\n        html_content = self.get_html_content()\n\n        # Clean the HTML content\n        cleaned_html = self.clean_html(html_content)\n\n        # Convert cleaned HTML to markdown\n        markdown_content = markdownify(\n            cleaned_html, heading_style=\"ATX\", bullets=\"-\", autolinks=True\n        )\n\n        # Additional cleanup of the markdown content\n        # Remove empty lines between list items\n        markdown_content = re.sub(r\"\\n\\n-\", \"\\n-\", markdown_content)\n        # Remove multiple consecutive empty lines\n        markdown_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_content)\n        # Remove any remaining CSS-like content\n        markdown_content = re.sub(r\"(\\{|\\}|\\[|\\])[^\\n]*\\n\", \"\", markdown_content)\n\n        # Save markdown content to file\n        markdown_path = self.export_markdown(markdown_content)\n        return markdown_path\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"HTML file not found: {self.source}\")\n    except Exception as e:\n        raise Exception(f\"Error processing HTML: {str(e)}\")\n</code></pre>"},{"location":"sources/digitization/native/word_docling/","title":"Word docling","text":""},{"location":"sources/digitization/native/word_docling/#Docs2KG.digitization.native.word_docling.DOCXMammoth","title":"<code>DOCXMammoth</code>","text":"<p>               Bases: <code>DigitizationBase</code></p> <p>DOCXDocling class for processing Word documents using mammoth.</p> Source code in <code>Docs2KG/digitization/native/word_docling.py</code> <pre><code>class DOCXMammoth(DigitizationBase):\n    \"\"\"\n    DOCXDocling class for processing Word documents using mammoth.\n    \"\"\"\n\n    def __init__(self, file_path: Path):\n        super().__init__(file_path=file_path, supported_formats=[\"docx\"])\n\n    @staticmethod\n    def validate_input(input_data: Union[str, Path]) -&gt; bool:\n        \"\"\"\n        Validate if the input is a valid DOCX file path.\n\n        Args:\n            input_data: Path to DOCX file (string or Path object)\n\n        Returns:\n            bool: True if input is valid DOCX file, False otherwise\n        \"\"\"\n        try:\n            path = Path(input_data)\n            return path.exists() and path.suffix.lower() == \".docx\"\n        except Exception as e:\n            logger.exception(f\"Error validating input: {str(e)}\")\n            return False\n\n    def export_markdown(self, content: str) -&gt; Path:\n        \"\"\"\n        Export content to markdown file.\n\n        Args:\n            content: The markdown content to export\n\n        Returns:\n            Path: Path to the generated markdown file\n        \"\"\"\n        markdown_path = self.output_dir / f\"{self.filename}.md\"\n        markdown_path.write_text(content, encoding=\"utf-8\")\n        return markdown_path\n\n    def process(self) -&gt; Path:\n        \"\"\"\n        Process DOCX document and generate markdown output.\n\n        Returns:\n            Path: Path to the generated markdown file\n\n        Raises:\n            ValueError: If input is not a valid DOCX file\n            FileNotFoundError: If DOCX file doesn't exist\n        \"\"\"\n        if not self.validate_input(self.file_path):\n            raise ValueError(\n                f\"Invalid input: {self.file_path}. Expected valid DOCX file\"\n            )\n\n        try:\n            # Convert DOCX to markdown using mammoth\n            with open(self.file_path, \"rb\") as docx_file:\n                result = mammoth.convert_to_markdown(docx_file)\n                markdown_content = result.value\n\n                # Log any conversion messages\n                if result.messages:\n                    for message in result.messages:\n                        logger.info(f\"Conversion message: {message}\")\n\n            # Save markdown content to file\n            markdown_path = self.export_markdown(markdown_content)\n            return markdown_path\n\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"DOCX file not found: {self.file_path}\")\n        except Exception as e:\n            raise Exception(f\"Error processing DOCX: {str(e)}\")\n\n    def __repr__(self) -&gt; str:\n        return f\"DOCXDocling(file_path='{self.file_path}')\"\n</code></pre>"},{"location":"sources/digitization/native/word_docling/#Docs2KG.digitization.native.word_docling.DOCXMammoth.export_markdown","title":"<code>export_markdown(content)</code>","text":"<p>Export content to markdown file.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The markdown content to export</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path to the generated markdown file</p> Source code in <code>Docs2KG/digitization/native/word_docling.py</code> <pre><code>def export_markdown(self, content: str) -&gt; Path:\n    \"\"\"\n    Export content to markdown file.\n\n    Args:\n        content: The markdown content to export\n\n    Returns:\n        Path: Path to the generated markdown file\n    \"\"\"\n    markdown_path = self.output_dir / f\"{self.filename}.md\"\n    markdown_path.write_text(content, encoding=\"utf-8\")\n    return markdown_path\n</code></pre>"},{"location":"sources/digitization/native/word_docling/#Docs2KG.digitization.native.word_docling.DOCXMammoth.process","title":"<code>process()</code>","text":"<p>Process DOCX document and generate markdown output.</p> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path to the generated markdown file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input is not a valid DOCX file</p> <code>FileNotFoundError</code> <p>If DOCX file doesn't exist</p> Source code in <code>Docs2KG/digitization/native/word_docling.py</code> <pre><code>def process(self) -&gt; Path:\n    \"\"\"\n    Process DOCX document and generate markdown output.\n\n    Returns:\n        Path: Path to the generated markdown file\n\n    Raises:\n        ValueError: If input is not a valid DOCX file\n        FileNotFoundError: If DOCX file doesn't exist\n    \"\"\"\n    if not self.validate_input(self.file_path):\n        raise ValueError(\n            f\"Invalid input: {self.file_path}. Expected valid DOCX file\"\n        )\n\n    try:\n        # Convert DOCX to markdown using mammoth\n        with open(self.file_path, \"rb\") as docx_file:\n            result = mammoth.convert_to_markdown(docx_file)\n            markdown_content = result.value\n\n            # Log any conversion messages\n            if result.messages:\n                for message in result.messages:\n                    logger.info(f\"Conversion message: {message}\")\n\n        # Save markdown content to file\n        markdown_path = self.export_markdown(markdown_content)\n        return markdown_path\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"DOCX file not found: {self.file_path}\")\n    except Exception as e:\n        raise Exception(f\"Error processing DOCX: {str(e)}\")\n</code></pre>"},{"location":"sources/digitization/native/word_docling/#Docs2KG.digitization.native.word_docling.DOCXMammoth.validate_input","title":"<code>validate_input(input_data)</code>  <code>staticmethod</code>","text":"<p>Validate if the input is a valid DOCX file path.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[str, Path]</code> <p>Path to DOCX file (string or Path object)</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if input is valid DOCX file, False otherwise</p> Source code in <code>Docs2KG/digitization/native/word_docling.py</code> <pre><code>@staticmethod\ndef validate_input(input_data: Union[str, Path]) -&gt; bool:\n    \"\"\"\n    Validate if the input is a valid DOCX file path.\n\n    Args:\n        input_data: Path to DOCX file (string or Path object)\n\n    Returns:\n        bool: True if input is valid DOCX file, False otherwise\n    \"\"\"\n    try:\n        path = Path(input_data)\n        return path.exists() and path.suffix.lower() == \".docx\"\n    except Exception as e:\n        logger.exception(f\"Error validating input: {str(e)}\")\n        return False\n</code></pre>"},{"location":"sources/kg_construction/base/","title":"Base","text":""},{"location":"sources/kg_construction/base/#Docs2KG.kg_construction.base.JSONEncoder","title":"<code>JSONEncoder</code>","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>Custom JSON encoder to handle numpy types and other special objects</p> Source code in <code>Docs2KG/kg_construction/base.py</code> <pre><code>class JSONEncoder(json.JSONEncoder):\n    \"\"\"Custom JSON encoder to handle numpy types and other special objects\"\"\"\n\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        if isinstance(obj, np.floating):\n            return float(obj)\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        if isinstance(obj, np.bool_):\n            return bool(obj)\n        if isinstance(obj, Path):\n            return str(obj)\n        if hasattr(obj, \"to_json\"):\n            return obj.to_json()\n        if hasattr(obj, \"to_dict\"):\n            return obj.to_dict()\n        return super().default(obj)\n</code></pre>"},{"location":"sources/kg_construction/base/#Docs2KG.kg_construction.base.KGConstructionBase","title":"<code>KGConstructionBase</code>","text":"Source code in <code>Docs2KG/kg_construction/base.py</code> <pre><code>class KGConstructionBase:\n    def __init__(self, project_id: str):\n        self.project_id = project_id\n        # create and set the project folder\n        self.project_folder = PROJECT_CONFIG.data.output_dir / \"projects\" / project_id\n        self.project_folder.mkdir(parents=True, exist_ok=True)\n\n        # create a sub folder for layout kg\n        layout_folder = self.project_folder / \"layout\"\n        layout_folder.mkdir(parents=True, exist_ok=True)\n        self.layout_folder = layout_folder\n        self.entity_type_list = []\n\n    def construct(self, docs):\n        raise NotImplementedError\n\n    def export_json(\n        self, data: Any, filename: Union[str, Path], ensure_ascii: bool = False\n    ) -&gt; Path:\n        \"\"\"\n        Export data to a JSON file with improved type handling.\n\n        Args:\n            data: The data to export\n            filename: Name of the output file\n            ensure_ascii: If False, allow non-ASCII characters in output\n\n        Returns:\n            Path: Path to the exported file\n\n        Raises:\n            IOError: If there's an error writing the file\n            TypeError: If an object type cannot be serialized\n        \"\"\"\n        try:\n            # Ensure filename has .json extension\n            if not str(filename).endswith(\".json\"):\n                filename = str(filename) + \".json\"\n\n            # Create output directory if it doesn't exist\n            self.project_folder.mkdir(parents=True, exist_ok=True)\n\n            output_path = self.project_folder / filename\n\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(data, f, cls=JSONEncoder, ensure_ascii=ensure_ascii, indent=4)\n\n            logger.info(f\"Successfully exported {filename} to {self.project_folder}\")\n            return output_path\n\n        except IOError as e:\n            logger.error(f\"Failed to write file {filename}: {str(e)}\")\n            raise\n\n        except TypeError as e:\n            logger.error(f\"Serialization error for {filename}: {str(e)}\")\n            raise\n</code></pre>"},{"location":"sources/kg_construction/base/#Docs2KG.kg_construction.base.KGConstructionBase.export_json","title":"<code>export_json(data, filename, ensure_ascii=False)</code>","text":"<p>Export data to a JSON file with improved type handling.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to export</p> required <code>filename</code> <code>Union[str, Path]</code> <p>Name of the output file</p> required <code>ensure_ascii</code> <code>bool</code> <p>If False, allow non-ASCII characters in output</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path to the exported file</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If there's an error writing the file</p> <code>TypeError</code> <p>If an object type cannot be serialized</p> Source code in <code>Docs2KG/kg_construction/base.py</code> <pre><code>def export_json(\n    self, data: Any, filename: Union[str, Path], ensure_ascii: bool = False\n) -&gt; Path:\n    \"\"\"\n    Export data to a JSON file with improved type handling.\n\n    Args:\n        data: The data to export\n        filename: Name of the output file\n        ensure_ascii: If False, allow non-ASCII characters in output\n\n    Returns:\n        Path: Path to the exported file\n\n    Raises:\n        IOError: If there's an error writing the file\n        TypeError: If an object type cannot be serialized\n    \"\"\"\n    try:\n        # Ensure filename has .json extension\n        if not str(filename).endswith(\".json\"):\n            filename = str(filename) + \".json\"\n\n        # Create output directory if it doesn't exist\n        self.project_folder.mkdir(parents=True, exist_ok=True)\n\n        output_path = self.project_folder / filename\n\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, cls=JSONEncoder, ensure_ascii=ensure_ascii, indent=4)\n\n        logger.info(f\"Successfully exported {filename} to {self.project_folder}\")\n        return output_path\n\n    except IOError as e:\n        logger.error(f\"Failed to write file {filename}: {str(e)}\")\n        raise\n\n    except TypeError as e:\n        logger.error(f\"Serialization error for {filename}: {str(e)}\")\n        raise\n</code></pre>"},{"location":"sources/kg_construction/layout_kg/layout_kg/","title":"Layout kg","text":""},{"location":"sources/kg_construction/layout_kg/layout_kg/#Docs2KG.kg_construction.layout_kg.layout_kg.LayoutKGConstruction","title":"<code>LayoutKGConstruction</code>","text":"<p>               Bases: <code>KGConstructionBase</code></p> <p>Constructs a layout knowledge graph from markdown documents. The output is a JSON file for each document containing layout elements.</p> Source code in <code>Docs2KG/kg_construction/layout_kg/layout_kg.py</code> <pre><code>class LayoutKGConstruction(KGConstructionBase):\n    \"\"\"\n    Constructs a layout knowledge graph from markdown documents.\n    The output is a JSON file for each document containing layout elements.\n    \"\"\"\n\n    def __init__(self, project_id: str):\n        super().__init__(project_id)\n        self.md = markdown.Markdown(extensions=[\"tables\", \"fenced_code\"])\n\n    def _parse_html_element(self, element: BeautifulSoup) -&gt; List[Dict[str, str]]:\n        \"\"\"\n        Parse an HTML element and extract layout information.\n\n        Args:\n            element: BeautifulSoup element from parsed markdown\n\n        Returns:\n            list: List of element information including id, text, and label\n        \"\"\"\n        elements = []\n\n        # Skip empty elements\n        if not element.text.strip():\n            return elements\n\n        # Generate element ID\n        element_id = f\"p_{str(uuid.uuid4())}\"\n\n        # Map HTML tags to layout labels\n        tag_to_label = {\n            \"h1\": \"H1\",\n            \"h2\": \"H2\",\n            \"h3\": \"H3\",\n            \"h4\": \"H4\",\n            \"h5\": \"H5\",\n            \"h6\": \"H6\",\n            \"p\": \"P\",\n            \"li\": \"LI\",\n            \"ol\": \"OL\",\n            \"ul\": \"UL\",\n            \"blockquote\": \"QUOTE\",\n            \"pre\": \"CODE\",\n            \"code\": \"CODE\",\n            \"table\": \"TABLE\",\n            \"tr\": \"TR\",\n            \"td\": \"TD\",\n            \"th\": \"TH\",\n        }\n\n        # Get the element's tag name\n        tag = element.name\n\n        # If it's a recognized tag, create an element entry\n        if tag in tag_to_label:\n            elements.append(\n                {\n                    \"id\": element_id,\n                    \"text\": element.get_text().strip(),\n                    \"label\": tag_to_label[tag],\n                    \"entities\": [],\n                    \"relations\": [],\n                }\n            )\n\n        # Recursively process child elements\n        for child in element.children:\n            if hasattr(child, \"name\") and child.name is not None:\n                elements.extend(self._parse_html_element(child))\n\n        return elements\n\n    def _process_document(self, content: str, filename: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Process a single markdown document and extract its layout elements.\n\n        Args:\n            content: Content of the markdown file\n            filename: Name of the document\n\n        Returns:\n            dict: Structured document information with layout elements\n        \"\"\"\n        # Convert markdown to HTML\n        html = self.md.convert(content)\n\n        # Parse HTML\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        # Extract elements\n        elements = []\n        for element in soup.find_all(recursive=False):\n            elements.extend(self._parse_html_element(element))\n\n        return {\n            \"filename\": filename,\n            \"data\": elements,\n            \"metadata\": {\n                \"title\": filename,\n            },\n        }\n\n    def construct(self, docs: List[Dict[str, str]]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Construct the layout knowledge graph from a list of documents.\n\n        Args:\n            docs: List of documents, where each document is a dict containing\n                 'content' and 'filename' keys\n\n        Returns:\n            dict: Layout knowledge graph containing all processed documents\n        \"\"\"\n        layout_kg = {}\n        # output a layout schema json\n        layout_schema = {\n            \"H1\": [\"H2\", \"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n            \"H2\": [\"H3\", \"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n            \"H3\": [\"H4\", \"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n            \"H4\": [\"H5\", \"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n            \"H5\": [\"H6\", \"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n            \"H6\": [\"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n            \"P\": [\"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n            \"LI\": [\"LI\", \"OL\", \"UL\", \"P\"],\n            \"OL\": [\"LI\", \"OL\", \"UL\", \"P\"],\n            \"UL\": [\"LI\", \"OL\", \"UL\", \"P\"],\n            \"QUOTE\": [\"P\", \"LI\", \"OL\", \"UL\", \"CODE\"],\n            \"CODE\": [\"CODE\"],\n            \"TABLE\": [\"TR\"],\n            \"TR\": [\"TD\", \"TH\"],\n            \"TD\": [\"P\"],\n            \"TH\": [\"P\"],\n        }\n        output_path = self.layout_folder / \"schema.json\"\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(layout_schema, f, indent=2, ensure_ascii=False)\n\n        for doc in docs:\n            content = doc[\"content\"]\n            filename = doc[\"filename\"]\n\n            # Process the document\n            doc_kg = self._process_document(content, filename)\n\n            # Save individual document KG\n            output_path = self.layout_folder / f\"{filename}.json\"\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(doc_kg, f, indent=2, ensure_ascii=False)\n\n            # Add to the complete KG\n            layout_kg[filename] = doc_kg\n\n        return layout_kg\n</code></pre>"},{"location":"sources/kg_construction/layout_kg/layout_kg/#Docs2KG.kg_construction.layout_kg.layout_kg.LayoutKGConstruction.construct","title":"<code>construct(docs)</code>","text":"<p>Construct the layout knowledge graph from a list of documents.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Dict[str, str]]</code> <p>List of documents, where each document is a dict containing  'content' and 'filename' keys</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Layout knowledge graph containing all processed documents</p> Source code in <code>Docs2KG/kg_construction/layout_kg/layout_kg.py</code> <pre><code>def construct(self, docs: List[Dict[str, str]]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Construct the layout knowledge graph from a list of documents.\n\n    Args:\n        docs: List of documents, where each document is a dict containing\n             'content' and 'filename' keys\n\n    Returns:\n        dict: Layout knowledge graph containing all processed documents\n    \"\"\"\n    layout_kg = {}\n    # output a layout schema json\n    layout_schema = {\n        \"H1\": [\"H2\", \"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n        \"H2\": [\"H3\", \"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n        \"H3\": [\"H4\", \"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n        \"H4\": [\"H5\", \"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n        \"H5\": [\"H6\", \"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n        \"H6\": [\"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n        \"P\": [\"P\", \"LI\", \"OL\", \"UL\", \"QUOTE\", \"CODE\", \"TABLE\"],\n        \"LI\": [\"LI\", \"OL\", \"UL\", \"P\"],\n        \"OL\": [\"LI\", \"OL\", \"UL\", \"P\"],\n        \"UL\": [\"LI\", \"OL\", \"UL\", \"P\"],\n        \"QUOTE\": [\"P\", \"LI\", \"OL\", \"UL\", \"CODE\"],\n        \"CODE\": [\"CODE\"],\n        \"TABLE\": [\"TR\"],\n        \"TR\": [\"TD\", \"TH\"],\n        \"TD\": [\"P\"],\n        \"TH\": [\"P\"],\n    }\n    output_path = self.layout_folder / \"schema.json\"\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(layout_schema, f, indent=2, ensure_ascii=False)\n\n    for doc in docs:\n        content = doc[\"content\"]\n        filename = doc[\"filename\"]\n\n        # Process the document\n        doc_kg = self._process_document(content, filename)\n\n        # Save individual document KG\n        output_path = self.layout_folder / f\"{filename}.json\"\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(doc_kg, f, indent=2, ensure_ascii=False)\n\n        # Add to the complete KG\n        layout_kg[filename] = doc_kg\n\n    return layout_kg\n</code></pre>"},{"location":"sources/kg_construction/metadata_kg/metadata_kg/","title":"Metadata kg","text":""},{"location":"sources/kg_construction/metadata_kg/metadata_kg/#Docs2KG.kg_construction.metadata_kg.metadata_kg.MetadataKGConstruction","title":"<code>MetadataKGConstruction</code>","text":"<p>               Bases: <code>KGConstructionBase</code></p> <p>The input should be a csv file with the following columns: - name: the name of the document - other columns: metadata fields, what we will do is to extract all unique values in each column and create a node for each value - and then create a relationship between the document and the metadata value - for columns is continuous, we will ignore it and put them as the property of the document node</p> Source code in <code>Docs2KG/kg_construction/metadata_kg/metadata_kg.py</code> <pre><code>class MetadataKGConstruction(KGConstructionBase):\n    \"\"\"\n    The input should be a csv file with the following columns:\n    - name: the name of the document\n    - other columns: metadata fields, what we will do is to extract all unique values in each column and create a node for each value\n    - and then create a relationship between the document and the metadata value\n    - for columns is continuous, we will ignore it and put them as the property of the document node\n    \"\"\"\n\n    def __init__(self, project_id: str):\n\n        super().__init__(project_id)\n        self.document_id_column = \"name\"\n        self.continuous_columns = []\n        self.categorical_columns = []\n\n    @staticmethod\n    def _is_continuous(series: pd.Series, threshold: float = 0.5) -&gt; bool:\n        \"\"\"\n        Determine if a column should be treated as continuous based on unique value ratio\n\n        Args:\n            series: pandas Series to check\n            threshold: ratio threshold to determine if continuous\n\n        Returns:\n            bool: True if the column should be treated as continuous\n        \"\"\"\n        unique_ratio = len(series.unique()) / len(series)\n        return unique_ratio &gt; threshold and pd.api.types.is_numeric_dtype(series)\n\n    def _identify_column_types(self, df: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Identify continuous and categorical columns in the dataframe\n\n        Args:\n            df: input dataframe\n        \"\"\"\n        for column in df.columns:\n            if column == self.document_id_column:\n                continue\n            if self._is_continuous(df[column]):\n                self.continuous_columns.append(column)\n            else:\n                self.categorical_columns.append(column)\n\n    def _create_document_nodes(self, df: pd.DataFrame) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Create document nodes with continuous properties\n\n        Args:\n            df: input dataframe\n\n        Returns:\n            List of document nodes with properties\n        \"\"\"\n        document_nodes = []\n        for _, row in df.iterrows():\n            properties = {\n                col: row[col] for col in self.continuous_columns if pd.notna(row[col])\n            }\n            node = {\n                \"id\": f\"doc_{row[self.document_id_column]}\",\n                \"type\": \"Document\",\n                \"properties\": {\n                    self.document_id_column: row[self.document_id_column],\n                    **properties,\n                },\n            }\n            document_nodes.append(node)\n        return document_nodes\n\n    def _create_metadata_nodes(self, df: pd.DataFrame) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Create nodes for unique categorical metadata values\n\n        Args:\n            df: input dataframe\n\n        Returns:\n            List of metadata value nodes\n        \"\"\"\n        metadata_nodes = []\n        for column in self.categorical_columns:\n            unique_values = df[column].dropna().unique()\n            for value in unique_values:\n                node = {\n                    \"id\": f\"{column}_{value}\",\n                    \"type\": column,\n                    \"properties\": {\"value\": value},\n                }\n                metadata_nodes.append(node)\n        return metadata_nodes\n\n    def _create_relationships(self, df: pd.DataFrame) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Create relationships between documents and their metadata values\n\n        Args:\n            df: input dataframe\n\n        Returns:\n            List of relationships\n        \"\"\"\n        relationships = []\n        for _, row in df.iterrows():\n            doc_id = f\"doc_{row[self.document_id_column]}\"\n            for column in self.categorical_columns:\n                if pd.notna(row[column]):\n                    relationship = {\n                        \"source\": doc_id,\n                        \"target\": f\"{column}_{row[column]}\",\n                        \"type\": f\"HAS_{column.upper()}\",\n                    }\n                    relationships.append(relationship)\n        return relationships\n\n    def construct(\n        self, docs: Union[str, pd.DataFrame], document_id_column: str = \"name\"\n    ) -&gt; Dict[str, List[Dict[str, Any]]]:\n        \"\"\"\n        Construct knowledge graph from document metadata\n\n        Args:\n            docs: Either path to CSV file or pandas DataFrame containing document metadata\n            document_id_column: Name of the column containing document IDs\n\n        Returns:\n            Dictionary containing nodes and relationships for the knowledge graph\n        \"\"\"\n        # Load data if string path provided\n        if isinstance(docs, str) or isinstance(docs, Path):\n            df = pd.read_csv(docs)\n        else:\n            df = docs.copy()\n\n        # remove unamed columns\n        df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n        # Validate required columns\n        if document_id_column not in df.columns:\n            raise ValueError(f\"Input data must contain '{document_id_column}' column\")\n        self.document_id_column = document_id_column\n        # Identify column types\n        self._identify_column_types(df)\n\n        # Create nodes and relationships\n        document_nodes = self._create_document_nodes(df)\n        metadata_nodes = self._create_metadata_nodes(df)\n        relationships = self._create_relationships(df)\n\n        metadata_kg = {\n            \"nodes\": document_nodes + metadata_nodes,\n            \"relationships\": relationships,\n        }\n\n        # export to metadata_kg.json\n        self.export_json(metadata_kg, \"metadata_kg.json\")\n\n        return metadata_kg\n</code></pre>"},{"location":"sources/kg_construction/metadata_kg/metadata_kg/#Docs2KG.kg_construction.metadata_kg.metadata_kg.MetadataKGConstruction.construct","title":"<code>construct(docs, document_id_column='name')</code>","text":"<p>Construct knowledge graph from document metadata</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[str, DataFrame]</code> <p>Either path to CSV file or pandas DataFrame containing document metadata</p> required <code>document_id_column</code> <code>str</code> <p>Name of the column containing document IDs</p> <code>'name'</code> <p>Returns:</p> Type Description <code>Dict[str, List[Dict[str, Any]]]</code> <p>Dictionary containing nodes and relationships for the knowledge graph</p> Source code in <code>Docs2KG/kg_construction/metadata_kg/metadata_kg.py</code> <pre><code>def construct(\n    self, docs: Union[str, pd.DataFrame], document_id_column: str = \"name\"\n) -&gt; Dict[str, List[Dict[str, Any]]]:\n    \"\"\"\n    Construct knowledge graph from document metadata\n\n    Args:\n        docs: Either path to CSV file or pandas DataFrame containing document metadata\n        document_id_column: Name of the column containing document IDs\n\n    Returns:\n        Dictionary containing nodes and relationships for the knowledge graph\n    \"\"\"\n    # Load data if string path provided\n    if isinstance(docs, str) or isinstance(docs, Path):\n        df = pd.read_csv(docs)\n    else:\n        df = docs.copy()\n\n    # remove unamed columns\n    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n    # Validate required columns\n    if document_id_column not in df.columns:\n        raise ValueError(f\"Input data must contain '{document_id_column}' column\")\n    self.document_id_column = document_id_column\n    # Identify column types\n    self._identify_column_types(df)\n\n    # Create nodes and relationships\n    document_nodes = self._create_document_nodes(df)\n    metadata_nodes = self._create_metadata_nodes(df)\n    relationships = self._create_relationships(df)\n\n    metadata_kg = {\n        \"nodes\": document_nodes + metadata_nodes,\n        \"relationships\": relationships,\n    }\n\n    # export to metadata_kg.json\n    self.export_json(metadata_kg, \"metadata_kg.json\")\n\n    return metadata_kg\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/base/","title":"Base","text":""},{"location":"sources/kg_construction/semantic_kg/base/#Docs2KG.kg_construction.semantic_kg.base.SemanticKGConstructionBase","title":"<code>SemanticKGConstructionBase</code>","text":"<p>               Bases: <code>KGConstructionBase</code></p> <p>Starting from the layout json, we will have several different ways to extract entities and relationships from the documents</p> <p>The task will typically into two parts: - Named Entity Recognition: extract entities from the text     - input can be: entity list, ontology, or just description - Relationship Extraction: extract relationships between entities</p> <p>Input will be an array of layout json files, output will be another json with entities and relationships extracted</p> Source code in <code>Docs2KG/kg_construction/semantic_kg/base.py</code> <pre><code>class SemanticKGConstructionBase(KGConstructionBase):\n    \"\"\"\n    Starting from the layout json, we will have several different ways to extract entities and relationships from the documents\n\n    The task will typically into two parts:\n    - Named Entity Recognition: extract entities from the text\n        - input can be: entity list, ontology, or just description\n    - Relationship Extraction: extract relationships between entities\n\n    Input will be an array of layout json files, output will be another json with entities and relationships extracted\n    \"\"\"\n\n    def __init__(self, project_id: str):\n        super().__init__(project_id)\n\n    @staticmethod\n    def load_layout_kg(layout_kg_path: Path) -&gt; dict:\n        \"\"\"\n        Load the layout knowledge graph from a file.\n\n        Args:\n            layout_kg_path: Path to the layout knowledge graph file\n\n        Returns:\n            dict: Layout knowledge graph\n        \"\"\"\n        if not layout_kg_path.exists():\n            raise FileNotFoundError(\n                f\"Layout knowledge graph not found at {layout_kg_path}\"\n            )\n        with open(layout_kg_path, \"r\") as file:\n            layout_kg = json.load(file)\n        return layout_kg\n\n    def load_entity_type(self):\n        # read from the entity list and ontology json\n        # update ontology json based on the entity list if needed\n        try:\n            entity_list_path = Path(PROJECT_CONFIG.semantic_kg.entity_list)\n            if not entity_list_path.exists():\n                raise FileNotFoundError(f\"Entity list not found at {entity_list_path}\")\n            with timer(logger, \"Loading entity list\"):\n                df = pd.read_csv(entity_list_path, sep=r\",(?=[^,]*$)\", engine=\"python\")\n            # get all entity types\n            entity_type_list = df[\"entity_type\"].unique()\n            # read from ontology json\n            ontology_json_path = Path(PROJECT_CONFIG.semantic_kg.ontology)\n            if not ontology_json_path.exists():\n                logger.warning(f\"Ontology json not found at {ontology_json_path}\")\n                ontology_entity_types = []\n            else:\n                with timer(logger, \"Loading ontology json\"):\n                    with open(ontology_json_path, \"r\") as f:\n                        ontology_json = json.load(f)\n                logger.info(f\"Ontology json: {ontology_json}\")\n                ontology = Ontology(**ontology_json)\n                # get all entity types from ontology\n                ontology_entity_types = ontology.entity_types\n\n            # combine the entity types from entity list and ontology\n            self.entity_type_list = list(\n                set(entity_type_list) | set(ontology_entity_types)\n            )\n            # update ontology json if needed\n            if len(self.entity_type_list) &gt; len(ontology_entity_types):\n                ontology.entity_types = self.entity_type_list\n                json_str = ontology.model_dump_json()\n                with open(ontology_json_path, \"w\") as f:\n                    f.write(json_str)\n        except Exception as e:\n            logger.exception(e)\n\n    @staticmethod\n    def update_layout_kg(layout_kg_path: Path, layout_kg: dict) -&gt; None:\n        \"\"\"\n        Update the layout knowledge graph in a file.\n\n        Args:\n            layout_kg_path: Path to the layout knowledge graph file\n            layout_kg: Layout knowledge graph to update\n        \"\"\"\n        with open(layout_kg_path, \"w\") as file:\n            json.dump(layout_kg, file, indent=2)\n\n    def construct_kg(self, input_data: Any) -&gt; None:\n        \"\"\"\n        Construct a semantic knowledge graph from input data.\n\n        Args:\n            input_data: Input data to construct the knowledge graph\n        \"\"\"\n        pass\n\n    @staticmethod\n    def unique_entities(entities):\n        unique_entities = []\n        seen_entities = set()\n        for entity in entities:\n            key = (\n                entity[\"start\"],\n                entity[\"end\"],\n                entity[\"text\"],\n                entity[\"label\"],\n            )\n            if key not in seen_entities:\n                unique_entities.append(entity)\n                seen_entities.add(key)\n        return unique_entities\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/base/#Docs2KG.kg_construction.semantic_kg.base.SemanticKGConstructionBase.construct_kg","title":"<code>construct_kg(input_data)</code>","text":"<p>Construct a semantic knowledge graph from input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data to construct the knowledge graph</p> required Source code in <code>Docs2KG/kg_construction/semantic_kg/base.py</code> <pre><code>def construct_kg(self, input_data: Any) -&gt; None:\n    \"\"\"\n    Construct a semantic knowledge graph from input data.\n\n    Args:\n        input_data: Input data to construct the knowledge graph\n    \"\"\"\n    pass\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/base/#Docs2KG.kg_construction.semantic_kg.base.SemanticKGConstructionBase.load_layout_kg","title":"<code>load_layout_kg(layout_kg_path)</code>  <code>staticmethod</code>","text":"<p>Load the layout knowledge graph from a file.</p> <p>Parameters:</p> Name Type Description Default <code>layout_kg_path</code> <code>Path</code> <p>Path to the layout knowledge graph file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Layout knowledge graph</p> Source code in <code>Docs2KG/kg_construction/semantic_kg/base.py</code> <pre><code>@staticmethod\ndef load_layout_kg(layout_kg_path: Path) -&gt; dict:\n    \"\"\"\n    Load the layout knowledge graph from a file.\n\n    Args:\n        layout_kg_path: Path to the layout knowledge graph file\n\n    Returns:\n        dict: Layout knowledge graph\n    \"\"\"\n    if not layout_kg_path.exists():\n        raise FileNotFoundError(\n            f\"Layout knowledge graph not found at {layout_kg_path}\"\n        )\n    with open(layout_kg_path, \"r\") as file:\n        layout_kg = json.load(file)\n    return layout_kg\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/base/#Docs2KG.kg_construction.semantic_kg.base.SemanticKGConstructionBase.update_layout_kg","title":"<code>update_layout_kg(layout_kg_path, layout_kg)</code>  <code>staticmethod</code>","text":"<p>Update the layout knowledge graph in a file.</p> <p>Parameters:</p> Name Type Description Default <code>layout_kg_path</code> <code>Path</code> <p>Path to the layout knowledge graph file</p> required <code>layout_kg</code> <code>dict</code> <p>Layout knowledge graph to update</p> required Source code in <code>Docs2KG/kg_construction/semantic_kg/base.py</code> <pre><code>@staticmethod\ndef update_layout_kg(layout_kg_path: Path, layout_kg: dict) -&gt; None:\n    \"\"\"\n    Update the layout knowledge graph in a file.\n\n    Args:\n        layout_kg_path: Path to the layout knowledge graph file\n        layout_kg: Layout knowledge graph to update\n    \"\"\"\n    with open(layout_kg_path, \"w\") as file:\n        json.dump(layout_kg, file, indent=2)\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/ner/ner_prompt_based/","title":"Ner prompt based","text":""},{"location":"sources/kg_construction/semantic_kg/ner/ner_prompt_based/#Docs2KG.kg_construction.semantic_kg.ner.ner_prompt_based.NERLLMPromptExtractor","title":"<code>NERLLMPromptExtractor</code>","text":"<p>               Bases: <code>SemanticKGConstructionBase</code></p> <p>Extract named entities using LLM and Entity Type List</p> Source code in <code>Docs2KG/kg_construction/semantic_kg/ner/ner_prompt_based.py</code> <pre><code>class NERLLMPromptExtractor(SemanticKGConstructionBase):\n    \"\"\"\n    Extract named entities using LLM and Entity Type List\n    \"\"\"\n\n    def __init__(\n        self,\n        project_id: str,\n        agent_name=\"phi3.5\",\n        agent_type=\"ollama\",\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize LLM NER Extractor\n\n        Args:\n            llm_entity_type_agent: Whether to use LLM for entity type judgement\n        \"\"\"\n        super().__init__(\n            project_id=project_id,\n        )\n\n        self.llm_ner_extract_agent = AgentManager(agent_name, agent_type, **kwargs)\n        self.entity_type_list = []\n        self.load_entity_type()\n\n    def extract_entities(self, text: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Extract entities from the given text, handling long texts by splitting into chunks\n\n        Args:\n            text: Text to extract entities from\n\n        Returns:\n            list: List of dictionaries containing entity information with format:\n            {\n                \"id\": str,          # Unique identifier for the entity\n                \"end\": int,         # End position in text\n                \"start\": int,       # Start position in text\n                \"text\": str,        # Matched text\n                \"label\": str,       # Entity type/label\n                \"confidence\": float # Confidence score\n            }\n        \"\"\"\n        if not text or (len(self.entity_type_list) == 0):\n            return []\n\n        # Split text into chunks, preserving the periods\n        text_chunks = [\n            chunk.strip() + \".\" for chunk in text.split(\".\") if chunk.strip()\n        ]\n\n        # Process each chunk while tracking overall character position\n        all_entities = []\n        current_position = 0\n\n        for chunk in text_chunks:\n            # Create prompt for current chunk\n            chunk_prompt = f\"\"\"\n            Extract entities from the given text:\n            {chunk.lower()}\n\n            It should be one of the following entity types:\n            {\", \".join(self.entity_type_list)}\n\n            Please output a list of entities in the following format via JSON:\n            [\n                    {{\n                        \"text\": \"entity text\",\n                        \"label\": \"entity type\",\n                        \"confidence\": 1.0\n                    }},\n                    ...\n                ]\n\n            entity text is the matched text\n            entity type is the label of the entity\n            confidence is the confidence score of the entity, it should be within [0.0, 1.0]\n            You should return it as an array of JSON objects.\n            \"\"\"\n\n            try:\n                # Process chunk\n                res = self.llm_ner_extract_agent.process_input(\n                    chunk_prompt, reset_session=True\n                )\n                res_json_str = res[\"response\"].strip()\n                # logger.info(f\"LLM response for chunk: {res_json_str}\")\n\n                entities_json = json.loads(res_json_str)\n                # if the json is a dict, convert it to a list\n                if isinstance(entities_json, dict):\n                    entities_json = [entities_json]\n\n                # Verify entities for this chunk\n                verified_chunk_entities = self.verify_output_entities(\n                    chunk.lower(), entities_json\n                )\n\n                logger.info(\n                    f\"Verified entities for chunk: {len(verified_chunk_entities)}. \\n{verified_chunk_entities}\"\n                )\n                # Adjust start and end positions based on current position in overall text\n                for entity in verified_chunk_entities:\n                    entity[\"start\"] += current_position\n                    entity[\"end\"] += current_position\n                    entity[\"method\"] = self.__class__.__name__\n\n                all_entities.extend(verified_chunk_entities)\n\n            except Exception as e:\n                logger.error(f\"Failed to extract entities from chunk: {str(e)}\")\n                logger.exception(e)\n                continue\n\n            # Update current position for next chunk\n            current_position += len(chunk)\n\n        logger.critical(\n            f\"All extracted and verified entities: {len(all_entities)}. \\n{all_entities}\"\n        )\n        return all_entities\n\n    def verify_output_entities(\n        self, text, entities: List[Dict[str, Any]]\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Verify the extracted entities, the start and end indices is correct\n\n        Args:\n            text: Text to extract entities from\n            entities: List of extracted entities\n\n        Returns:\n            list: List of verified entities\n        \"\"\"\n        verified_entities = []\n        for entity in entities:\n            if entity[\"label\"] not in self.entity_type_list:\n                logger.info(\n                    f\"Dropping entity: {entity} for entity type: {entity['label']}\"\n                )\n                logger.warning(f\"Entity type {entity['label']} not in entity type list\")\n                continue\n            entity[\"start\"], entity[\"end\"] = self.locate_text_start_end(text, entity)\n            if entity[\"start\"] is None or entity[\"end\"] is None:\n                logger.error(f\"Failed to locate entity: {entity}\")\n                continue\n            entity[\"start\"], entity[\"end\"] = int(entity[\"start\"]), int(entity[\"end\"])\n            # add a unique id for the entity\n            entity[\"id\"] = (\n                f\"ner-llm-{hash(entity['text'] + str(entity['start']) + str(entity['end']) + entity['label'])}\"\n            )\n            verified_entities.append(entity)\n        return verified_entities\n\n    @staticmethod\n    def verify_entity_position(start, end, text, entity):\n        \"\"\"\n        Verify the entity position in the text\n\n        Args:\n            start: Start index of the entity\n            end: End index of the entity\n            text: Text to extract entities from\n            entity: Extracted entity\n\n        Returns:\n            bool: True if the entity position is correct, False otherwise\n        \"\"\"\n        try:\n            return text[start:end] == entity[\"text\"]\n        except Exception as e:\n            logger.error(f\"Failed to verify entity position: {str(e)}\")\n            return False\n\n    @staticmethod\n    def locate_text_start_end(text, entity):\n        \"\"\"\n        Locate the start and end index of the entity in the text\n\n        Args:\n            text: Text to extract entities from\n            entity: Extracted entity\n\n        Returns:\n            tuple: Start and end index of the entity\n        \"\"\"\n        try:\n            start = text.find(entity[\"text\"])\n            # if start == -1, which means the entity is not found in the text\n            if start == -1:\n                return None, None\n            end = start + len(entity[\"text\"])\n            return start, end\n        except Exception as e:\n            logger.error(f\"Failed to locate entity in text: {str(e)}\")\n            return None, None\n\n    def construct_kg(self, input_data: List[Path]) -&gt; None:\n        \"\"\"\n        Construct a semantic knowledge graph from input data.\n\n        Args:\n            input_data: Input data to construct the knowledge graph\n        \"\"\"\n        logger.info(\n            f\"Extracting entities from {len(input_data)} layout knowledge graphs\"\n        )\n        for layout_kg_path in input_data:\n            if not layout_kg_path.exists():\n                logger.error(f\"Layout knowledge graph not found at {layout_kg_path}\")\n                continue\n            layout_kg = self.load_layout_kg(layout_kg_path)\n\n            if \"data\" not in layout_kg:\n                logger.error(f\"Document data not found in {layout_kg_path}\")\n                continue\n\n            for item in layout_kg[\"data\"]:\n                if \"text\" not in item:\n                    logger.error(f\"Text not found in document item: {item}\")\n                    continue\n                text = item[\"text\"]\n                entities = self.extract_entities(text)\n                # extend the extracted entities to the layout knowledge graph\n                item[\"entities\"].extend(entities)\n                # remove duplicated entities based on start and end positions, text and label\n                item[\"entities\"] = self.unique_entities(item[\"entities\"])\n\n            self.update_layout_kg(layout_kg_path, layout_kg)\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/ner/ner_prompt_based/#Docs2KG.kg_construction.semantic_kg.ner.ner_prompt_based.NERLLMPromptExtractor.__init__","title":"<code>__init__(project_id, agent_name='phi3.5', agent_type='ollama', **kwargs)</code>","text":"<p>Initialize LLM NER Extractor</p> <p>Parameters:</p> Name Type Description Default <code>llm_entity_type_agent</code> <p>Whether to use LLM for entity type judgement</p> required Source code in <code>Docs2KG/kg_construction/semantic_kg/ner/ner_prompt_based.py</code> <pre><code>def __init__(\n    self,\n    project_id: str,\n    agent_name=\"phi3.5\",\n    agent_type=\"ollama\",\n    **kwargs,\n):\n    \"\"\"\n    Initialize LLM NER Extractor\n\n    Args:\n        llm_entity_type_agent: Whether to use LLM for entity type judgement\n    \"\"\"\n    super().__init__(\n        project_id=project_id,\n    )\n\n    self.llm_ner_extract_agent = AgentManager(agent_name, agent_type, **kwargs)\n    self.entity_type_list = []\n    self.load_entity_type()\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/ner/ner_prompt_based/#Docs2KG.kg_construction.semantic_kg.ner.ner_prompt_based.NERLLMPromptExtractor.construct_kg","title":"<code>construct_kg(input_data)</code>","text":"<p>Construct a semantic knowledge graph from input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>List[Path]</code> <p>Input data to construct the knowledge graph</p> required Source code in <code>Docs2KG/kg_construction/semantic_kg/ner/ner_prompt_based.py</code> <pre><code>def construct_kg(self, input_data: List[Path]) -&gt; None:\n    \"\"\"\n    Construct a semantic knowledge graph from input data.\n\n    Args:\n        input_data: Input data to construct the knowledge graph\n    \"\"\"\n    logger.info(\n        f\"Extracting entities from {len(input_data)} layout knowledge graphs\"\n    )\n    for layout_kg_path in input_data:\n        if not layout_kg_path.exists():\n            logger.error(f\"Layout knowledge graph not found at {layout_kg_path}\")\n            continue\n        layout_kg = self.load_layout_kg(layout_kg_path)\n\n        if \"data\" not in layout_kg:\n            logger.error(f\"Document data not found in {layout_kg_path}\")\n            continue\n\n        for item in layout_kg[\"data\"]:\n            if \"text\" not in item:\n                logger.error(f\"Text not found in document item: {item}\")\n                continue\n            text = item[\"text\"]\n            entities = self.extract_entities(text)\n            # extend the extracted entities to the layout knowledge graph\n            item[\"entities\"].extend(entities)\n            # remove duplicated entities based on start and end positions, text and label\n            item[\"entities\"] = self.unique_entities(item[\"entities\"])\n\n        self.update_layout_kg(layout_kg_path, layout_kg)\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/ner/ner_prompt_based/#Docs2KG.kg_construction.semantic_kg.ner.ner_prompt_based.NERLLMPromptExtractor.extract_entities","title":"<code>extract_entities(text)</code>","text":"<p>Extract entities from the given text, handling long texts by splitting into chunks</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to extract entities from</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[Dict[str, Any]]</code> <p>List of dictionaries containing entity information with format:</p> <code>List[Dict[str, Any]]</code> <p>{ \"id\": str,          # Unique identifier for the entity \"end\": int,         # End position in text \"start\": int,       # Start position in text \"text\": str,        # Matched text \"label\": str,       # Entity type/label \"confidence\": float # Confidence score</p> <code>List[Dict[str, Any]]</code> <p>}</p> Source code in <code>Docs2KG/kg_construction/semantic_kg/ner/ner_prompt_based.py</code> <pre><code>def extract_entities(self, text: str) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Extract entities from the given text, handling long texts by splitting into chunks\n\n    Args:\n        text: Text to extract entities from\n\n    Returns:\n        list: List of dictionaries containing entity information with format:\n        {\n            \"id\": str,          # Unique identifier for the entity\n            \"end\": int,         # End position in text\n            \"start\": int,       # Start position in text\n            \"text\": str,        # Matched text\n            \"label\": str,       # Entity type/label\n            \"confidence\": float # Confidence score\n        }\n    \"\"\"\n    if not text or (len(self.entity_type_list) == 0):\n        return []\n\n    # Split text into chunks, preserving the periods\n    text_chunks = [\n        chunk.strip() + \".\" for chunk in text.split(\".\") if chunk.strip()\n    ]\n\n    # Process each chunk while tracking overall character position\n    all_entities = []\n    current_position = 0\n\n    for chunk in text_chunks:\n        # Create prompt for current chunk\n        chunk_prompt = f\"\"\"\n        Extract entities from the given text:\n        {chunk.lower()}\n\n        It should be one of the following entity types:\n        {\", \".join(self.entity_type_list)}\n\n        Please output a list of entities in the following format via JSON:\n        [\n                {{\n                    \"text\": \"entity text\",\n                    \"label\": \"entity type\",\n                    \"confidence\": 1.0\n                }},\n                ...\n            ]\n\n        entity text is the matched text\n        entity type is the label of the entity\n        confidence is the confidence score of the entity, it should be within [0.0, 1.0]\n        You should return it as an array of JSON objects.\n        \"\"\"\n\n        try:\n            # Process chunk\n            res = self.llm_ner_extract_agent.process_input(\n                chunk_prompt, reset_session=True\n            )\n            res_json_str = res[\"response\"].strip()\n            # logger.info(f\"LLM response for chunk: {res_json_str}\")\n\n            entities_json = json.loads(res_json_str)\n            # if the json is a dict, convert it to a list\n            if isinstance(entities_json, dict):\n                entities_json = [entities_json]\n\n            # Verify entities for this chunk\n            verified_chunk_entities = self.verify_output_entities(\n                chunk.lower(), entities_json\n            )\n\n            logger.info(\n                f\"Verified entities for chunk: {len(verified_chunk_entities)}. \\n{verified_chunk_entities}\"\n            )\n            # Adjust start and end positions based on current position in overall text\n            for entity in verified_chunk_entities:\n                entity[\"start\"] += current_position\n                entity[\"end\"] += current_position\n                entity[\"method\"] = self.__class__.__name__\n\n            all_entities.extend(verified_chunk_entities)\n\n        except Exception as e:\n            logger.error(f\"Failed to extract entities from chunk: {str(e)}\")\n            logger.exception(e)\n            continue\n\n        # Update current position for next chunk\n        current_position += len(chunk)\n\n    logger.critical(\n        f\"All extracted and verified entities: {len(all_entities)}. \\n{all_entities}\"\n    )\n    return all_entities\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/ner/ner_prompt_based/#Docs2KG.kg_construction.semantic_kg.ner.ner_prompt_based.NERLLMPromptExtractor.locate_text_start_end","title":"<code>locate_text_start_end(text, entity)</code>  <code>staticmethod</code>","text":"<p>Locate the start and end index of the entity in the text</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>Text to extract entities from</p> required <code>entity</code> <p>Extracted entity</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>Start and end index of the entity</p> Source code in <code>Docs2KG/kg_construction/semantic_kg/ner/ner_prompt_based.py</code> <pre><code>@staticmethod\ndef locate_text_start_end(text, entity):\n    \"\"\"\n    Locate the start and end index of the entity in the text\n\n    Args:\n        text: Text to extract entities from\n        entity: Extracted entity\n\n    Returns:\n        tuple: Start and end index of the entity\n    \"\"\"\n    try:\n        start = text.find(entity[\"text\"])\n        # if start == -1, which means the entity is not found in the text\n        if start == -1:\n            return None, None\n        end = start + len(entity[\"text\"])\n        return start, end\n    except Exception as e:\n        logger.error(f\"Failed to locate entity in text: {str(e)}\")\n        return None, None\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/ner/ner_prompt_based/#Docs2KG.kg_construction.semantic_kg.ner.ner_prompt_based.NERLLMPromptExtractor.verify_entity_position","title":"<code>verify_entity_position(start, end, text, entity)</code>  <code>staticmethod</code>","text":"<p>Verify the entity position in the text</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <p>Start index of the entity</p> required <code>end</code> <p>End index of the entity</p> required <code>text</code> <p>Text to extract entities from</p> required <code>entity</code> <p>Extracted entity</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the entity position is correct, False otherwise</p> Source code in <code>Docs2KG/kg_construction/semantic_kg/ner/ner_prompt_based.py</code> <pre><code>@staticmethod\ndef verify_entity_position(start, end, text, entity):\n    \"\"\"\n    Verify the entity position in the text\n\n    Args:\n        start: Start index of the entity\n        end: End index of the entity\n        text: Text to extract entities from\n        entity: Extracted entity\n\n    Returns:\n        bool: True if the entity position is correct, False otherwise\n    \"\"\"\n    try:\n        return text[start:end] == entity[\"text\"]\n    except Exception as e:\n        logger.error(f\"Failed to verify entity position: {str(e)}\")\n        return False\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/ner/ner_prompt_based/#Docs2KG.kg_construction.semantic_kg.ner.ner_prompt_based.NERLLMPromptExtractor.verify_output_entities","title":"<code>verify_output_entities(text, entities)</code>","text":"<p>Verify the extracted entities, the start and end indices is correct</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>Text to extract entities from</p> required <code>entities</code> <code>List[Dict[str, Any]]</code> <p>List of extracted entities</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[Dict[str, Any]]</code> <p>List of verified entities</p> Source code in <code>Docs2KG/kg_construction/semantic_kg/ner/ner_prompt_based.py</code> <pre><code>def verify_output_entities(\n    self, text, entities: List[Dict[str, Any]]\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Verify the extracted entities, the start and end indices is correct\n\n    Args:\n        text: Text to extract entities from\n        entities: List of extracted entities\n\n    Returns:\n        list: List of verified entities\n    \"\"\"\n    verified_entities = []\n    for entity in entities:\n        if entity[\"label\"] not in self.entity_type_list:\n            logger.info(\n                f\"Dropping entity: {entity} for entity type: {entity['label']}\"\n            )\n            logger.warning(f\"Entity type {entity['label']} not in entity type list\")\n            continue\n        entity[\"start\"], entity[\"end\"] = self.locate_text_start_end(text, entity)\n        if entity[\"start\"] is None or entity[\"end\"] is None:\n            logger.error(f\"Failed to locate entity: {entity}\")\n            continue\n        entity[\"start\"], entity[\"end\"] = int(entity[\"start\"]), int(entity[\"end\"])\n        # add a unique id for the entity\n        entity[\"id\"] = (\n            f\"ner-llm-{hash(entity['text'] + str(entity['start']) + str(entity['end']) + entity['label'])}\"\n        )\n        verified_entities.append(entity)\n    return verified_entities\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/ner/ner_spacy_match/","title":"Ner spacy match","text":""},{"location":"sources/kg_construction/semantic_kg/ner/ner_spacy_match/#Docs2KG.kg_construction.semantic_kg.ner.ner_spacy_match.NERSpacyMatcher","title":"<code>NERSpacyMatcher</code>","text":"<p>               Bases: <code>SemanticKGConstructionBase</code></p> <p>To get this working, need to run: python -m spacy download en_core_web_sm first</p> Source code in <code>Docs2KG/kg_construction/semantic_kg/ner/ner_spacy_match.py</code> <pre><code>class NERSpacyMatcher(SemanticKGConstructionBase):\n    \"\"\"\n    To get this working, need to run: python -m spacy download en_core_web_sm first\n\n    \"\"\"\n\n    def __init__(\n        self,\n        project_id: str,\n        agent_name: str = \"phi3.5\",\n        agent_type: str = \"ollama\",\n    ):\n        super().__init__(project_id)\n        # Load SpaCy model (use a smaller model for speed)\n        self.nlp = spacy.load(\"en_core_web_sm\")\n        self.matcher = Matcher(self.nlp.vocab)\n        self.entity_dict = {}\n        self.load_entity_list()\n        self.llm_judgement_agent = NERLLMJudge(agent_name, agent_type)\n\n    def load_entity_list(self):\n        try:\n            entity_list_path = Path(PROJECT_CONFIG.semantic_kg.entity_list)\n            if not entity_list_path.exists():\n                raise FileNotFoundError(f\"Entity list not found at {entity_list_path}\")\n            with timer(logger, \"Loading entity list\"):\n                df = pd.read_csv(entity_list_path, sep=r\",(?=[^,]*$)\", engine=\"python\")\n            self.entity_dict = dict(zip(df[\"entity\"], df[\"entity_type\"]))\n            self._initialize_patterns()\n\n        except Exception as e:\n            logger.error(f\"Error loading entity list: {e}\")\n            return\n\n    def _initialize_patterns(self):\n        \"\"\"\n        Convert entity dictionary to SpaCy patterns\n        Handles both single-word and multi-word entities\n        \"\"\"\n        patterns = []\n\n        for entity_text, entity_type in self.entity_dict.items():\n            # Convert entity text to lowercase\n            entity_lower = entity_text.lower()\n\n            # Split entity text into tokens\n            tokens = entity_lower.split()\n\n            # Create pattern for exact matching\n            pattern = [{\"LOWER\": token} for token in tokens]\n\n            # Add pattern to matcher with unique ID\n            pattern_id = f\"{entity_type}_{hash(entity_lower)}\"\n            self.matcher.add(pattern_id, [pattern])\n\n            # Store mapping of pattern_id to original entity text and type\n            patterns.append(\n                {\n                    \"id\": pattern_id,\n                    \"text\": entity_text,\n                    \"type\": entity_type,\n                    \"pattern\": pattern,\n                }\n            )\n\n        self.patterns = patterns\n\n    def extract_entities(self, text: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Given the text, find the case-insensitive match of the entities in the entity dict.\n        Uses SpaCy Matcher to find the entities in the text first.\n\n        Args:\n            text (str): The input text to search for entities\n\n        Returns:\n            list: List of dictionaries containing entity information with format:\n            {\n                \"id\": str,          # Unique identifier for the entity\n                \"end\": int,         # End position in text\n                \"start\": int,       # Start position in text\n                \"text\": str,        # Matched text\n                \"label\": str,       # Entity type/label\n                \"confidence\": float # Confidence score\n            }\n        \"\"\"\n        if not text or not self.entity_dict:\n            return []\n        text = text.lower()\n        # Process text with SpaCy\n        doc = self.nlp(text)\n\n        # Find matches using the matcher\n        matches = self.matcher(doc)\n\n        # Convert matches to our format\n        results = []\n        for match_id, start, end in matches:\n            # Get the matched span\n            span = doc[start:end]\n\n            # Get the original text from the span\n            matched_text = span.text\n\n            # Find corresponding pattern info\n            pattern_id = self.nlp.vocab.strings[match_id]\n            entity_info = next(\n                (p for p in self.patterns if p[\"id\"] == pattern_id), None\n            )\n\n            if entity_info:\n                # Create match entry\n                if not self._validate_match(doc, start, end):\n                    continue\n\n                is_correct = self.llm_judgement_agent.judge(\n                    ner=matched_text, ner_type=entity_info[\"type\"], text=text\n                )\n                if not is_correct:\n                    continue\n\n                match = {\n                    \"id\": f\"ner-spacy-{hash(matched_text + str(start) + str(end))}-{str(uuid4())}\",\n                    \"start\": span.start_char,\n                    \"end\": span.end_char,\n                    \"text\": matched_text,\n                    \"label\": entity_info[\"type\"],\n                    \"confidence\": (\n                        0.95\n                        if matched_text.lower() == entity_info[\"text\"].lower()\n                        else 0.9\n                    ),\n                    \"method\": self.__class__.__name__,\n                }\n                results.append(match)\n\n        # Sort results by start position\n        results.sort(key=lambda x: x[\"start\"])\n\n        logger.info(f\"Extracted entities: {results} for text: {text}\")\n        return results\n\n    @staticmethod\n    def _validate_match(doc, start, end):\n        \"\"\"\n        Validate if a match is at proper word boundaries\n\n        Args:\n            doc: SpaCy Doc object\n            start: Start token index\n            end: End token index\n\n        Returns:\n            bool: Whether the match is valid\n        \"\"\"\n\n        # Check if the span is at token boundaries\n        if start &gt; 0 and doc[start - 1].is_alpha:\n            return False\n        if end &lt; len(doc) and doc[end].is_alpha:\n            return False\n\n        return True\n\n    def construct_kg(self, input_data: List[Path]) -&gt; None:\n        \"\"\"\n        Construct a semantic knowledge graph from input data.\n\n        Args:\n            input_data: Input data to construct the knowledge graph\n        \"\"\"\n        # Process each document\n        for doc in tqdm(input_data, desc=\"Processing documents\"):\n            # Extract entities from the document text\n            if not doc.exists():\n                logger.error(f\"Document not found at {doc}\")\n                continue\n            logger.info(f\"Processing document: {doc}\")\n            layout_kg = self.load_layout_kg(doc)\n            if \"data\" not in layout_kg:\n                logger.error(f\"Document data not found in {doc}\")\n                continue\n            for item in layout_kg[\"data\"]:\n                if \"text\" not in item:\n                    logger.error(f\"Text not found in document item: {item}\")\n                    continue\n                text = item[\"text\"]\n                entities = self.extract_entities(text)\n                # expand the item entities list with the extracted entities\n                item[\"entities\"].extend(entities)\n                # then remove duplicated entities based on start and end positions, text and label\n                item[\"entities\"] = self.unique_entities(item[\"entities\"])\n\n            self.update_layout_kg(doc, layout_kg)\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/ner/ner_spacy_match/#Docs2KG.kg_construction.semantic_kg.ner.ner_spacy_match.NERSpacyMatcher.construct_kg","title":"<code>construct_kg(input_data)</code>","text":"<p>Construct a semantic knowledge graph from input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>List[Path]</code> <p>Input data to construct the knowledge graph</p> required Source code in <code>Docs2KG/kg_construction/semantic_kg/ner/ner_spacy_match.py</code> <pre><code>def construct_kg(self, input_data: List[Path]) -&gt; None:\n    \"\"\"\n    Construct a semantic knowledge graph from input data.\n\n    Args:\n        input_data: Input data to construct the knowledge graph\n    \"\"\"\n    # Process each document\n    for doc in tqdm(input_data, desc=\"Processing documents\"):\n        # Extract entities from the document text\n        if not doc.exists():\n            logger.error(f\"Document not found at {doc}\")\n            continue\n        logger.info(f\"Processing document: {doc}\")\n        layout_kg = self.load_layout_kg(doc)\n        if \"data\" not in layout_kg:\n            logger.error(f\"Document data not found in {doc}\")\n            continue\n        for item in layout_kg[\"data\"]:\n            if \"text\" not in item:\n                logger.error(f\"Text not found in document item: {item}\")\n                continue\n            text = item[\"text\"]\n            entities = self.extract_entities(text)\n            # expand the item entities list with the extracted entities\n            item[\"entities\"].extend(entities)\n            # then remove duplicated entities based on start and end positions, text and label\n            item[\"entities\"] = self.unique_entities(item[\"entities\"])\n\n        self.update_layout_kg(doc, layout_kg)\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/ner/ner_spacy_match/#Docs2KG.kg_construction.semantic_kg.ner.ner_spacy_match.NERSpacyMatcher.extract_entities","title":"<code>extract_entities(text)</code>","text":"<p>Given the text, find the case-insensitive match of the entities in the entity dict. Uses SpaCy Matcher to find the entities in the text first.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to search for entities</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[Dict[str, Any]]</code> <p>List of dictionaries containing entity information with format:</p> <code>List[Dict[str, Any]]</code> <p>{ \"id\": str,          # Unique identifier for the entity \"end\": int,         # End position in text \"start\": int,       # Start position in text \"text\": str,        # Matched text \"label\": str,       # Entity type/label \"confidence\": float # Confidence score</p> <code>List[Dict[str, Any]]</code> <p>}</p> Source code in <code>Docs2KG/kg_construction/semantic_kg/ner/ner_spacy_match.py</code> <pre><code>def extract_entities(self, text: str) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Given the text, find the case-insensitive match of the entities in the entity dict.\n    Uses SpaCy Matcher to find the entities in the text first.\n\n    Args:\n        text (str): The input text to search for entities\n\n    Returns:\n        list: List of dictionaries containing entity information with format:\n        {\n            \"id\": str,          # Unique identifier for the entity\n            \"end\": int,         # End position in text\n            \"start\": int,       # Start position in text\n            \"text\": str,        # Matched text\n            \"label\": str,       # Entity type/label\n            \"confidence\": float # Confidence score\n        }\n    \"\"\"\n    if not text or not self.entity_dict:\n        return []\n    text = text.lower()\n    # Process text with SpaCy\n    doc = self.nlp(text)\n\n    # Find matches using the matcher\n    matches = self.matcher(doc)\n\n    # Convert matches to our format\n    results = []\n    for match_id, start, end in matches:\n        # Get the matched span\n        span = doc[start:end]\n\n        # Get the original text from the span\n        matched_text = span.text\n\n        # Find corresponding pattern info\n        pattern_id = self.nlp.vocab.strings[match_id]\n        entity_info = next(\n            (p for p in self.patterns if p[\"id\"] == pattern_id), None\n        )\n\n        if entity_info:\n            # Create match entry\n            if not self._validate_match(doc, start, end):\n                continue\n\n            is_correct = self.llm_judgement_agent.judge(\n                ner=matched_text, ner_type=entity_info[\"type\"], text=text\n            )\n            if not is_correct:\n                continue\n\n            match = {\n                \"id\": f\"ner-spacy-{hash(matched_text + str(start) + str(end))}-{str(uuid4())}\",\n                \"start\": span.start_char,\n                \"end\": span.end_char,\n                \"text\": matched_text,\n                \"label\": entity_info[\"type\"],\n                \"confidence\": (\n                    0.95\n                    if matched_text.lower() == entity_info[\"text\"].lower()\n                    else 0.9\n                ),\n                \"method\": self.__class__.__name__,\n            }\n            results.append(match)\n\n    # Sort results by start position\n    results.sort(key=lambda x: x[\"start\"])\n\n    logger.info(f\"Extracted entities: {results} for text: {text}\")\n    return results\n</code></pre>"},{"location":"sources/kg_construction/semantic_kg/ontology/entity_type_llm/","title":"Entity type llm","text":""},{"location":"sources/kg_construction/semantic_kg/ontology/entity_type_llm/#Docs2KG.kg_construction.semantic_kg.ontology.entity_type_llm.EntityTypesLLMGenerator","title":"<code>EntityTypesLLMGenerator</code>","text":"<p>               Bases: <code>SemanticKGConstructionBase</code></p> <p>It will query the - project description - content of the pdf (total text) - current entity type list</p> <p>And then ask the LLM to generate the entity types.</p> <p>Also, we will combine generated entity types with the current entity type list. And update the entity type list within the folder</p> Source code in <code>Docs2KG/kg_construction/semantic_kg/ontology/entity_type_llm.py</code> <pre><code>class EntityTypesLLMGenerator(SemanticKGConstructionBase):\n    \"\"\"\n    It will query the\n    - project description\n    - content of the pdf (total text)\n    - current entity type list\n\n    And then ask the LLM to generate the entity types.\n\n    Also, we will combine generated entity types with the current entity type list.\n    And update the entity type list within the folder\n\n    \"\"\"\n\n    def __init__(\n        self, project_id: str, agent_name=\"phi3.5\", agent_type=\"ollama\", **kwargs\n    ):\n        super().__init__(project_id)\n        self.ontology_agent = AgentManager(agent_name, agent_type, **kwargs)\n        # first load project description\n        self.project_description = self.load_project_description()\n        self.load_entity_type()\n\n    @staticmethod\n    def load_project_description():\n        project_description_path = Path(PROJECT_CONFIG.semantic_kg.domain_description)\n        if not project_description_path.exists():\n            raise FileNotFoundError(\n                f\"Project description not found at {project_description_path}\"\n            )\n        with open(project_description_path, \"r\") as file:\n            project_description = file.read()\n        return project_description\n\n    def generate_entity_types(\n        self,\n        content: Optional[str] = None,\n    ):\n        prompt = f\"\"\"You are a expert to generate entity types based on the following project description:\n                    '{self.project_description}'\n                    and the content of the pdf:\n                    '{content}'\n\n                    The current entity types are:\n                    {self.entity_type_list}\n\n                    Please generate some new related entity types based on the information above\n                    **mainly based on the content of pdf**\n                    Do not generate repeated entity types.\n\n                    Generated entity type should be short, concise, representative and concise.\n\n                    Return in JSON format with key entity_types,\n                    and value as a list of entity types which will be a string of the entity type, separated by comma.\n\n                    If the current entity types already cover most of the entities, you can return an empty list.\n                    \"\"\"\n\n        response = self.ontology_agent.process_input(prompt, reset_session=True)\n        res_json_str = response[\"response\"]\n        logger.debug(f\"LLM response: {res_json_str}\")\n        new_entity_types = self.extract_entity_types(res_json_str)\n        logger.critical(f\"New entity types: {new_entity_types}\")\n        return new_entity_types\n\n    @staticmethod\n    def extract_entity_types(res_json_str):\n        try:\n            res_json_str = res_json_str.strip()\n            res_json = json.loads(res_json_str)\n            entity_types_list_str = res_json.get(\"entity_types\", \"\")\n            if isinstance(entity_types_list_str, list):\n                entity_types = entity_types_list_str\n            else:\n                entity_types_list_str = entity_types_list_str.strip()\n                entity_types = entity_types_list_str.split(\",\")\n            entity_types = [entity_type.strip() for entity_type in entity_types]\n            return entity_types\n        except Exception as e:\n            logger.error(f\"Failed to extract entity types from response: {str(e)}\")\n            return None\n\n    def construct_ontology(self):\n        new_entity_types = self.generate_entity_types()\n        logger.critical(f\"New entity types: {new_entity_types}\")\n        if new_entity_types:\n            self.update_ontology(new_entity_types)\n\n    @staticmethod\n    def update_ontology(new_entity_types):\n        ontology_json_path = Path(PROJECT_CONFIG.semantic_kg.ontology)\n        if not ontology_json_path.exists():\n            logger.warning(f\"Ontology json not found at {ontology_json_path}\")\n            ontology_entity_types = []\n        else:\n            with timer(logger, \"Loading ontology json\"):\n                with open(ontology_json_path, \"r\") as f:\n                    ontology_json = json.load(f)\n            logger.info(f\"Ontology json: {ontology_json}\")\n            ontology = Ontology(**ontology_json)\n            ontology_entity_types = ontology.entity_types\n\n        new_entity_types = list(set(new_entity_types) | set(ontology_entity_types))\n        ontology.entity_types = new_entity_types\n        json_str = ontology.model_dump_json()\n        with open(ontology_json_path, \"w\") as f:\n            f.write(json_str)\n</code></pre>"},{"location":"sources/utils/config/","title":"Config","text":""},{"location":"sources/utils/config/#Docs2KG.utils.config.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>Docs2KG/utils/config.py</code> <pre><code>class Config(BaseModel):\n    openai: AgentOpenAIConfig\n    ollama: AgentOLLAMAConfig\n    huggingface: AgentHuggingFaceConfig\n    llamacpp: AgentLlamaCppConfig\n    data: DataConfig\n    semantic_kg: SemanticKGConfig\n\n    @classmethod\n    def from_yaml(cls, yaml_path: Path) -&gt; \"Config\":\n        \"\"\"Load configuration from a YAML file.\"\"\"\n        if not yaml_path.exists():\n            raise FileNotFoundError(f\"Config file not found: {yaml_path}\")\n\n        with open(yaml_path) as f:\n            config_dict = safe_load(f)\n\n        return cls(**config_dict)\n</code></pre>"},{"location":"sources/utils/config/#Docs2KG.utils.config.Config.from_yaml","title":"<code>from_yaml(yaml_path)</code>  <code>classmethod</code>","text":"<p>Load configuration from a YAML file.</p> Source code in <code>Docs2KG/utils/config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, yaml_path: Path) -&gt; \"Config\":\n    \"\"\"Load configuration from a YAML file.\"\"\"\n    if not yaml_path.exists():\n        raise FileNotFoundError(f\"Config file not found: {yaml_path}\")\n\n    with open(yaml_path) as f:\n        config_dict = safe_load(f)\n\n    return cls(**config_dict)\n</code></pre>"},{"location":"sources/utils/config/#Docs2KG.utils.config.get_config","title":"<code>get_config()</code>  <code>cached</code>","text":"<p>Get the configuration singleton. The lru_cache decorator ensures this is only created once and reused.</p> Source code in <code>Docs2KG/utils/config.py</code> <pre><code>@lru_cache()\ndef get_config() -&gt; Config:\n    \"\"\"\n    Get the configuration singleton.\n    The lru_cache decorator ensures this is only created once and reused.\n    \"\"\"\n    try:\n        config = Config.from_yaml(CONFIG_FILE)\n        logger.info(\"Configuration loaded successfully\")\n        return config\n    except Exception as e:\n        logger.error(f\"Failed to load configuration: {e}\")\n        raise\n</code></pre>"},{"location":"sources/utils/constants/","title":"Constants","text":""},{"location":"sources/utils/empty_check/","title":"Empty check","text":""},{"location":"sources/utils/models/","title":"Models","text":""},{"location":"sources/utils/neo4j_loader/","title":"Neo4j loader","text":""},{"location":"sources/utils/neo4j_loader/#Docs2KG.utils.neo4j_loader.Neo4jTransformer","title":"<code>Neo4jTransformer</code>","text":"Source code in <code>Docs2KG/utils/neo4j_loader.py</code> <pre><code>class Neo4jTransformer:\n    def __init__(\n        self,\n        project_id: str,\n        uri: str,\n        username: str,\n        password: str,\n        database: Optional[str] = None,\n        reset_database: bool = False,\n    ):\n        \"\"\"Initialize the transformer with Neo4j connection details\"\"\"\n        self.project_id = project_id\n        self.driver = GraphDatabase.driver(uri, auth=basic_auth(username, password))\n        self.database = database\n        self.reset_database = reset_database\n        self.layout_schema_path = (\n            PROJECT_CONFIG.data.output_dir\n            / \"projects\"\n            / project_id\n            / \"layout\"\n            / \"schema.json\"\n        )\n        self.layout_schema = self._load_layout_schema()\n        self.header_stack = []  # Track header hierarchy\n        self.current_file_id = None\n        if self.reset_database:\n            with self.driver.session(database=self.database) as session:\n                session.run(\"MATCH (n) DETACH DELETE n\")\n\n    def _load_layout_schema(self) -&gt; Dict:\n        \"\"\"Load layout schema from file\"\"\"\n        with open(self.layout_schema_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n    def load_metadata_kg(self, session):\n        \"\"\"\n        Loads a metadata knowledge graph from a JSON file into Neo4j.\n        - Creates/merges a :Project node with the given project_id if not existing.\n        - Loads all 'nodes' into :Node label. Each node's 'properties' are flattened into separate node properties.\n        - Loads all 'relationships' using a :RELATES_TO relationship.\n        \"\"\"\n\n        # 1. Check the metadata_kg file exists.\n        metadata_kg_path = (\n            PROJECT_CONFIG.data.output_dir\n            / \"projects\"\n            / self.project_id\n            / \"metadata_kg.json\"\n        )\n        if not metadata_kg_path.exists():\n            logger.error(f\"Metadata knowledge graph not found at {metadata_kg_path}\")\n            return None\n\n        # first check if the project node exists\n        project_query = \"\"\"\n        MATCH (p:Project {id: $project_id})\n        RETURN p\n        \"\"\"\n\n        result = session.run(project_query, project_id=self.project_id)\n        project_node = result.single()\n        if project_node is not None:\n            logger.info(f\"Project {self.project_id} already exists. Skipping load.\")\n            return\n\n        # 2. Merge the :Project node to ensure the label is created and the node is present.\n        #    If it already exists, we can decide whether to skip or proceed.\n        project_merge_query = \"\"\"\n        MERGE (p:Project {id: $project_id})\n        ON CREATE SET p.createdAt = timestamp()\n        RETURN p\n        \"\"\"\n        session.run(project_merge_query, project_id=self.project_id)\n\n        # 3. Load the JSON content\n        with open(metadata_kg_path, \"r\", encoding=\"utf-8\") as f:\n            metadata_kg = json.load(f)\n\n        # 4. Insert all nodes\n        #    We flatten node[\"properties\"] into the node so that each key in `properties`\n        #    is stored as a direct property on the node. If you prefer to store them as a single\n        #    JSON string, see the alternative approach commented below.\n\n        with timer(logger, \"Loading metadata knowledge graph: Nodes\"):\n            for node in metadata_kg[\"nodes\"]:\n                # node[\"properties\"] must be a dict of {string_key -&gt; scalar_value}\n                # so that `SET n += $props` can distribute them as node properties.\n\n                # Example: for {\"ANumber\":144050}, this becomes n.ANumber = 144050\n                node_props = node[\"properties\"] if \"properties\" in node else {}\n\n                # If you want each node's type to be an actual Neo4j label, you can do:\n                #   create_node_cypher = \"CREATE (n:\" + node[\"type\"] + \" {id: $id}) SET n += $props\"\n                #   but that depends on your domain model.\n\n                # add project_id to node properties\n                node_props[\"project_id\"] = self.project_id\n                create_node_cypher = (\n                    \"\"\"\n                    CREATE (n:\"\"\"\n                    + node.get(\"type\", \"Node\")\n                    + \"\"\"{id: $id, type: $type})\n                SET n += $props\n                \"\"\"\n                )\n                session.run(\n                    create_node_cypher,\n                    id=node[\"id\"],\n                    label=node.get(\"type\", \"Node\"),  # fallback empty string if no type\n                    type=node.get(\"type\", \"\"),  # fallback empty string if no type\n                    props=node_props,\n                )\n\n        with timer(logger, \"Loading metadata knowledge graph: Relationships\"):\n            for relation in metadata_kg[\"relationships\"]:\n                # cypher query to match the id of the start and end nodes\n                # and create a relationship between them\n                create_relationship_cypher = \"\"\"\n                MATCH (start), (end)\n                WHERE start.id = $start_id AND end.id = $end_id\n                CREATE (start)-[:RELATES_TO $props]-&gt;(end)\n                \"\"\"\n                # add project_id to relationship properties\n                relation_props = (\n                    relation[\"properties\"] if \"properties\" in relation else {}\n                )\n                relation_props[\"project_id\"] = self.project_id\n                session.run(\n                    create_relationship_cypher,\n                    start_id=relation[\"source\"],\n                    end_id=relation[\"target\"],\n                    props=relation_props,\n                )\n\n        logger.info(f\"Metadata knowledge graph loaded for project {self.project_id}.\")\n        return True\n\n    def close(self):\n        \"\"\"Close the Neo4j driver\"\"\"\n        self.driver.close()\n\n    def merge_entities(self):\n        \"\"\"Merge entities with same label and text within the same project\"\"\"\n        with self.driver.session(database=self.database) as session:\n            # First, find duplicate entities (same label, text, and project)\n            find_duplicates_query = \"\"\"\n            MATCH (e1)\n            WHERE e1.text IS NOT NULL AND e1.method IS NOT NULL  // ensure it's an entity\n            WITH e1.text as text, labels(e1)[0] as label, e1.project_id as project_id,\n                 collect(e1) as entities, count(*) as count\n            WHERE count &gt; 1\n            RETURN text, label, project_id, entities\n            \"\"\"\n\n            duplicates = session.run(find_duplicates_query)\n\n            for record in duplicates:\n\n                entities = record[\"entities\"]\n\n                # Keep first entity as primary\n                primary_entity = entities[0]\n                duplicate_entities = entities[1:]\n\n                # For each duplicate\n                for dup_entity in duplicate_entities:\n                    # First, redirect all incoming relationships\n                    session.run(\n                        \"\"\"\n                    MATCH (dup) WHERE elementId(dup) = $dup_id\n                    MATCH (primary) WHERE elementId(primary) = $primary_id\n                    MATCH (dup)&lt;-[r]-()\n                    WITH dup, r, startNode(r) as start_node, primary, properties(r) as props\n                    CREATE (start_node)-[new_r:HAS_ENTITY]-&gt;(primary)\n                    SET new_r = props\n                    WITH dup, r\n                    DELETE r\n                    \"\"\",\n                        dup_id=dup_entity.element_id,\n                        primary_id=primary_entity.element_id,\n                    )\n\n                    # Then, redirect all outgoing relationships\n                    session.run(\n                        \"\"\"\n                    MATCH (dup) WHERE elementId(dup) = $dup_id\n                    MATCH (primary) WHERE elementId(primary) = $primary_id\n                    MATCH (dup)-[r]-&gt;()\n                    WITH dup, r, endNode(r) as end_node, primary, properties(r) as props\n                    CREATE (primary)-[new_r:RELATES_TO]-&gt;(end_node)\n                    SET new_r = props\n                    WITH dup, r\n                    DELETE r\n                    \"\"\",\n                        dup_id=dup_entity.element_id,\n                        primary_id=primary_entity.element_id,\n                    )\n\n                    # Finally, delete duplicate node\n                    session.run(\n                        \"\"\"\n                    MATCH (dup) WHERE elementId(dup) = $dup_id\n                    DELETE dup\n                    \"\"\",\n                        dup_id=dup_entity.element_id,\n                    )\n\n            # Create uniqueness constraint if it doesn't exist\n            try:\n                session.run(\n                    \"\"\"\n                CREATE CONSTRAINT unique_entity IF NOT EXISTS\n                FOR (e:Entity)\n                REQUIRE (e.text, e.label, e.project_id) IS UNIQUE\n                \"\"\"\n                )\n            except Exception as e:\n                # Handle older Neo4j versions or other constraint errors\n                print(f\"Warning: Could not create constraint - {str(e)}\")\n\n    def transform_and_load(self, input_path: Path):\n        \"\"\"Transform and load data into Neo4j\"\"\"\n        if \"layout\" not in str(input_path):\n            logger.warning(\"Input file is not a layout knowledge graph\")\n            return\n        layout_json = json.load(open(input_path, \"r\", encoding=\"utf-8\"))\n\n        with self.driver.session(database=self.database) as session:\n            # Load metadata knowledge graph\n            self.load_metadata_kg(session)\n\n            # Create file node with unique ID\n            self.current_file_id = f\"{self.project_id}_{layout_json['filename']}\"\n            file_props = {\n                \"id\": self.current_file_id,\n                \"filename\": layout_json[\"filename\"],\n                \"project_id\": self.project_id,\n            }\n\n            session.run(\n                \"\"\"\n                CREATE (f:File $props)\n                \"\"\",\n                props=file_props,\n            )\n\n            # Reset state\n            self.header_stack = []\n\n            # Process layout structure\n            self._create_layout(session, layout=layout_json[\"data\"])\n\n            # Process entities and relations\n            for item in layout_json[\"data\"]:\n                self._process_entities(session, item)\n                self._process_relations(session, item)\n\n            # Merge duplicate entities after all data is loaded\n            self.merge_entities()\n\n    def _find_parent_node(\n        self, session, current_item: Dict, previous_items: List[Dict]\n    ) -&gt; Optional[str]:\n        \"\"\"Find the appropriate parent node ID based on document structure rules\"\"\"\n        current_label = current_item[\"label\"]\n\n        # If it's a header, handle header hierarchy\n        if current_label.startswith(\"H\"):\n            current_level = int(current_label[1])\n\n            # Update header stack\n            while (\n                self.header_stack and int(self.header_stack[-1][0][1]) &gt;= current_level\n            ):\n                self.header_stack.pop()\n\n            if not self.header_stack:\n                return None  # Connect to File node\n\n            return self.header_stack[-1][1]  # Return last valid header's ID\n\n        # For non-header nodes, check schema rules\n        if previous_items:\n            prev_item = previous_items[-1]\n            prev_label = prev_item[\"label\"]\n\n            # If previous label can contain current label according to schema\n            if (\n                prev_label in self.layout_schema\n                and current_label in self.layout_schema[prev_label]\n            ):\n                return prev_item[\"id\"]\n\n            # If there's a header context\n            if self.header_stack:\n                return self.header_stack[-1][1]\n\n        return None  # Default to connecting to File node\n\n    def _create_layout(self, session, layout: List[Dict]):\n        \"\"\"Create layout structure with proper hierarchical relationships\"\"\"\n        processed_items = []\n\n        for idx, item in enumerate(layout):\n            item_props = {\n                \"id\": item[\"id\"],\n                \"text\": item.get(\"text\", \"\"),\n                \"sequence\": idx,\n                \"project_id\": self.project_id,\n            }\n\n            label = self.sanitize_label(item.get(\"label\", \"Item\"))\n\n            # Find parent node\n            parent_id = self._find_parent_node(session, item, processed_items)\n\n            if parent_id:\n                # Create node with relationship to parent\n                query = f\"\"\"\n                MATCH (p) WHERE p.id = $parent_id\n                CREATE (p)-[:CONTAINS]-&gt;(n:{label} $props)\n                RETURN n\n                \"\"\"\n                session.run(query, parent_id=parent_id, props=item_props)\n            else:\n                # Create node with relationship to file\n                query = f\"\"\"\n                MATCH (f:File {{id: $file_id}})\n                CREATE (f)-[:CONTAINS]-&gt;(n:{label} $props)\n                RETURN n\n                \"\"\"\n                session.run(query, file_id=self.current_file_id, props=item_props)\n\n            # Update header stack if needed\n            if label.startswith(\"H\"):\n                self.header_stack.append((label, item[\"id\"]))\n\n            # Add to processed items\n            processed_items.append(item)\n\n            # Create NEXT relationship with previous node at same level\n            if processed_items and len(processed_items) &gt; 1:\n                prev_item = processed_items[-2]\n                if prev_item[\"label\"] == item[\"label\"]:\n                    session.run(\n                        \"\"\"\n                        MATCH (p), (n)\n                        WHERE p.id = $prev_id AND n.id = $curr_id\n                        CREATE (p)-[:NEXT]-&gt;(n)\n                        \"\"\",\n                        prev_id=prev_item[\"id\"],\n                        curr_id=item[\"id\"],\n                    )\n\n    def _process_entities(self, session, item: Dict):\n        \"\"\"Process entities for an item\"\"\"\n        for entity in item.get(\"entities\", []):\n            entity_props = {\n                \"id\": entity.get(\"id\", \"\"),\n                \"text\": entity.get(\"text\", \"\"),\n                \"confidence\": entity.get(\"confidence\", 0.0),\n                \"start\": entity.get(\"start\", 0),\n                \"end\": entity.get(\"end\", 0),\n                \"method\": entity.get(\"method\", \"\"),\n                \"project_id\": self.project_id,\n            }\n\n            entity_label = self.sanitize_label(entity.get(\"label\", \"Entity\"))\n\n            session.run(\n                f\"\"\"\n                MATCH (p) WHERE p.id = $item_id\n                CREATE (p)-[:HAS_ENTITY]-&gt;(e:{entity_label} $props)\n                \"\"\",\n                item_id=item[\"id\"],\n                props=entity_props,\n            )\n\n    def _process_relations(self, session, item: Dict):\n        \"\"\"Process relations for an item\"\"\"\n        for relation in item.get(\"relations\", []):\n            relation_props = {\n                \"type\": relation.get(\"type\", \"RELATES_TO\"),\n                \"confidence\": relation.get(\"confidence\", 0.0),\n                \"project_id\": self.project_id,\n            }\n\n            if \"source_id\" in relation and \"target_id\" in relation:\n                session.run(\n                    \"\"\"\n                    MATCH (s), (t)\n                    WHERE s.id = $source_id AND t.id = $target_id\n                    CREATE (s)-[r:RELATES_TO $props]-&gt;(t)\n                    \"\"\",\n                    source_id=relation[\"source_id\"],\n                    target_id=relation[\"target_id\"],\n                    props=relation_props,\n                )\n\n    @staticmethod\n    def sanitize_label(label: str) -&gt; str:\n        \"\"\"\n        Sanitize label for Neo4j:\n        - Replaces spaces and hyphens with underscores\n        - Converts to uppercase\n        - Moves any leading numbers to the end of the label\n        \"\"\"\n        # First sanitize special characters\n        sanitized = label.replace(\" \", \"_\").replace(\"-\", \"_\").upper()\n\n        # If label starts with a number, move leading numbers to end\n        if sanitized and sanitized[0].isdigit():\n            leading_nums = \"\"\n            i = 0\n            while i &lt; len(sanitized) and (\n                sanitized[i].isdigit() or sanitized[i] == \"_\"\n            ):\n                leading_nums += sanitized[i]\n                i += 1\n            return f\"{sanitized[i:]}{leading_nums}\" if i &lt; len(sanitized) else sanitized\n\n        return sanitized\n\n    def get_document_structure(self, file_id: str):\n        \"\"\"Get the document structure as a tree\"\"\"\n        with self.driver.session(database=self.database) as session:\n            query = \"\"\"\n            MATCH path = (f:File {id: $file_id})-[r:CONTAINS|NEXT*]-&gt;(n)\n            RETURN path\n            ORDER BY n.sequence\n            \"\"\"\n            results = session.run(query, file_id=file_id)\n            return [record[\"path\"] for record in results]\n\n    def export(self):\n        \"\"\"Export the Neo4j database to a JSON file that can be reimported later\n\n        Returns a JSON structure containing nodes and relationships with their properties,\n        labels, and types preserved.\n        \"\"\"\n        with self.driver.session(database=self.database) as session:\n            # Get all nodes with their labels and properties\n            nodes_query = \"\"\"\n            MATCH (n)\n            RETURN collect({\n                id: id(n),\n                labels: labels(n),\n                properties: properties(n)\n            }) as nodes\n            \"\"\"\n\n            # Get all relationships with their types and properties\n            rels_query = \"\"\"\n            MATCH ()-[r]-&gt;()\n            RETURN collect({\n                id: id(r),\n                type: type(r),\n                properties: properties(r),\n                startNode: id(startNode(r)),\n                endNode: id(endNode(r))\n            }) as relationships\n            \"\"\"\n\n            nodes = session.run(nodes_query).single()[\"nodes\"]\n            relationships = session.run(rels_query).single()[\"relationships\"]\n\n            export_data = {\"nodes\": nodes, \"relationships\": relationships}\n\n            # Write to file\n            neo4j_export = (\n                PROJECT_CONFIG.data.output_dir\n                / \"projects\"\n                / self.project_id\n                / \"neo4j_export.json\"\n            )\n            with open(neo4j_export, \"w\") as f:\n                json.dump(export_data, f, indent=2)\n\n        logger.info(\"Exported Neo4j database to neo4j_export.json\")\n        return export_data\n\n    def import_from_json(self, filepath):\n        \"\"\"Import the Neo4j database from a previously exported JSON file\"\"\"\n        with open(filepath, \"r\") as f:\n            json_data = json.load(f)\n\n        with self.driver.session(database=self.database) as session:\n            # First create all nodes with unique identifiers\n            node_mapping = {}  # To store mapping between old and new elementIds\n\n            for node in json_data[\"nodes\"]:\n                labels = \":\".join(node[\"labels\"])\n                properties = dict(node[\"properties\"])\n\n                # Create node and return its elementId\n                create_node_query = f\"\"\"\n                CREATE (n:{labels})\n                SET n = $properties\n                RETURN elementId(n) as new_id\n                \"\"\"\n                result = session.run(create_node_query, properties=properties)\n                new_id = result.single()[\"new_id\"]\n                node_mapping[node[\"id\"]] = new_id\n\n            # Then create all relationships using the new elementIds\n            for rel in json_data[\"relationships\"]:\n                rel_type = rel[\"type\"]\n                properties = dict(rel[\"properties\"])\n                start_id = node_mapping[rel[\"startNode\"]]\n                end_id = node_mapping[rel[\"endNode\"]]\n\n                create_rel_query = f\"\"\"\n                MATCH (start), (end)\n                WHERE elementId(start) = $start_id AND elementId(end) = $end_id\n                CREATE (start)-[r:{rel_type}]-&gt;(end)\n                SET r = $properties\n                \"\"\"\n                session.run(\n                    create_rel_query,\n                    start_id=start_id,\n                    end_id=end_id,\n                    properties=properties,\n                )\n\n        logger.info(f\"Imported Neo4j database from {filepath}\")\n</code></pre>"},{"location":"sources/utils/neo4j_loader/#Docs2KG.utils.neo4j_loader.Neo4jTransformer.__init__","title":"<code>__init__(project_id, uri, username, password, database=None, reset_database=False)</code>","text":"<p>Initialize the transformer with Neo4j connection details</p> Source code in <code>Docs2KG/utils/neo4j_loader.py</code> <pre><code>def __init__(\n    self,\n    project_id: str,\n    uri: str,\n    username: str,\n    password: str,\n    database: Optional[str] = None,\n    reset_database: bool = False,\n):\n    \"\"\"Initialize the transformer with Neo4j connection details\"\"\"\n    self.project_id = project_id\n    self.driver = GraphDatabase.driver(uri, auth=basic_auth(username, password))\n    self.database = database\n    self.reset_database = reset_database\n    self.layout_schema_path = (\n        PROJECT_CONFIG.data.output_dir\n        / \"projects\"\n        / project_id\n        / \"layout\"\n        / \"schema.json\"\n    )\n    self.layout_schema = self._load_layout_schema()\n    self.header_stack = []  # Track header hierarchy\n    self.current_file_id = None\n    if self.reset_database:\n        with self.driver.session(database=self.database) as session:\n            session.run(\"MATCH (n) DETACH DELETE n\")\n</code></pre>"},{"location":"sources/utils/neo4j_loader/#Docs2KG.utils.neo4j_loader.Neo4jTransformer.close","title":"<code>close()</code>","text":"<p>Close the Neo4j driver</p> Source code in <code>Docs2KG/utils/neo4j_loader.py</code> <pre><code>def close(self):\n    \"\"\"Close the Neo4j driver\"\"\"\n    self.driver.close()\n</code></pre>"},{"location":"sources/utils/neo4j_loader/#Docs2KG.utils.neo4j_loader.Neo4jTransformer.export","title":"<code>export()</code>","text":"<p>Export the Neo4j database to a JSON file that can be reimported later</p> <p>Returns a JSON structure containing nodes and relationships with their properties, labels, and types preserved.</p> Source code in <code>Docs2KG/utils/neo4j_loader.py</code> <pre><code>def export(self):\n    \"\"\"Export the Neo4j database to a JSON file that can be reimported later\n\n    Returns a JSON structure containing nodes and relationships with their properties,\n    labels, and types preserved.\n    \"\"\"\n    with self.driver.session(database=self.database) as session:\n        # Get all nodes with their labels and properties\n        nodes_query = \"\"\"\n        MATCH (n)\n        RETURN collect({\n            id: id(n),\n            labels: labels(n),\n            properties: properties(n)\n        }) as nodes\n        \"\"\"\n\n        # Get all relationships with their types and properties\n        rels_query = \"\"\"\n        MATCH ()-[r]-&gt;()\n        RETURN collect({\n            id: id(r),\n            type: type(r),\n            properties: properties(r),\n            startNode: id(startNode(r)),\n            endNode: id(endNode(r))\n        }) as relationships\n        \"\"\"\n\n        nodes = session.run(nodes_query).single()[\"nodes\"]\n        relationships = session.run(rels_query).single()[\"relationships\"]\n\n        export_data = {\"nodes\": nodes, \"relationships\": relationships}\n\n        # Write to file\n        neo4j_export = (\n            PROJECT_CONFIG.data.output_dir\n            / \"projects\"\n            / self.project_id\n            / \"neo4j_export.json\"\n        )\n        with open(neo4j_export, \"w\") as f:\n            json.dump(export_data, f, indent=2)\n\n    logger.info(\"Exported Neo4j database to neo4j_export.json\")\n    return export_data\n</code></pre>"},{"location":"sources/utils/neo4j_loader/#Docs2KG.utils.neo4j_loader.Neo4jTransformer.get_document_structure","title":"<code>get_document_structure(file_id)</code>","text":"<p>Get the document structure as a tree</p> Source code in <code>Docs2KG/utils/neo4j_loader.py</code> <pre><code>def get_document_structure(self, file_id: str):\n    \"\"\"Get the document structure as a tree\"\"\"\n    with self.driver.session(database=self.database) as session:\n        query = \"\"\"\n        MATCH path = (f:File {id: $file_id})-[r:CONTAINS|NEXT*]-&gt;(n)\n        RETURN path\n        ORDER BY n.sequence\n        \"\"\"\n        results = session.run(query, file_id=file_id)\n        return [record[\"path\"] for record in results]\n</code></pre>"},{"location":"sources/utils/neo4j_loader/#Docs2KG.utils.neo4j_loader.Neo4jTransformer.import_from_json","title":"<code>import_from_json(filepath)</code>","text":"<p>Import the Neo4j database from a previously exported JSON file</p> Source code in <code>Docs2KG/utils/neo4j_loader.py</code> <pre><code>def import_from_json(self, filepath):\n    \"\"\"Import the Neo4j database from a previously exported JSON file\"\"\"\n    with open(filepath, \"r\") as f:\n        json_data = json.load(f)\n\n    with self.driver.session(database=self.database) as session:\n        # First create all nodes with unique identifiers\n        node_mapping = {}  # To store mapping between old and new elementIds\n\n        for node in json_data[\"nodes\"]:\n            labels = \":\".join(node[\"labels\"])\n            properties = dict(node[\"properties\"])\n\n            # Create node and return its elementId\n            create_node_query = f\"\"\"\n            CREATE (n:{labels})\n            SET n = $properties\n            RETURN elementId(n) as new_id\n            \"\"\"\n            result = session.run(create_node_query, properties=properties)\n            new_id = result.single()[\"new_id\"]\n            node_mapping[node[\"id\"]] = new_id\n\n        # Then create all relationships using the new elementIds\n        for rel in json_data[\"relationships\"]:\n            rel_type = rel[\"type\"]\n            properties = dict(rel[\"properties\"])\n            start_id = node_mapping[rel[\"startNode\"]]\n            end_id = node_mapping[rel[\"endNode\"]]\n\n            create_rel_query = f\"\"\"\n            MATCH (start), (end)\n            WHERE elementId(start) = $start_id AND elementId(end) = $end_id\n            CREATE (start)-[r:{rel_type}]-&gt;(end)\n            SET r = $properties\n            \"\"\"\n            session.run(\n                create_rel_query,\n                start_id=start_id,\n                end_id=end_id,\n                properties=properties,\n            )\n\n    logger.info(f\"Imported Neo4j database from {filepath}\")\n</code></pre>"},{"location":"sources/utils/neo4j_loader/#Docs2KG.utils.neo4j_loader.Neo4jTransformer.load_metadata_kg","title":"<code>load_metadata_kg(session)</code>","text":"<p>Loads a metadata knowledge graph from a JSON file into Neo4j. - Creates/merges a :Project node with the given project_id if not existing. - Loads all 'nodes' into :Node label. Each node's 'properties' are flattened into separate node properties. - Loads all 'relationships' using a :RELATES_TO relationship.</p> Source code in <code>Docs2KG/utils/neo4j_loader.py</code> <pre><code>def load_metadata_kg(self, session):\n    \"\"\"\n    Loads a metadata knowledge graph from a JSON file into Neo4j.\n    - Creates/merges a :Project node with the given project_id if not existing.\n    - Loads all 'nodes' into :Node label. Each node's 'properties' are flattened into separate node properties.\n    - Loads all 'relationships' using a :RELATES_TO relationship.\n    \"\"\"\n\n    # 1. Check the metadata_kg file exists.\n    metadata_kg_path = (\n        PROJECT_CONFIG.data.output_dir\n        / \"projects\"\n        / self.project_id\n        / \"metadata_kg.json\"\n    )\n    if not metadata_kg_path.exists():\n        logger.error(f\"Metadata knowledge graph not found at {metadata_kg_path}\")\n        return None\n\n    # first check if the project node exists\n    project_query = \"\"\"\n    MATCH (p:Project {id: $project_id})\n    RETURN p\n    \"\"\"\n\n    result = session.run(project_query, project_id=self.project_id)\n    project_node = result.single()\n    if project_node is not None:\n        logger.info(f\"Project {self.project_id} already exists. Skipping load.\")\n        return\n\n    # 2. Merge the :Project node to ensure the label is created and the node is present.\n    #    If it already exists, we can decide whether to skip or proceed.\n    project_merge_query = \"\"\"\n    MERGE (p:Project {id: $project_id})\n    ON CREATE SET p.createdAt = timestamp()\n    RETURN p\n    \"\"\"\n    session.run(project_merge_query, project_id=self.project_id)\n\n    # 3. Load the JSON content\n    with open(metadata_kg_path, \"r\", encoding=\"utf-8\") as f:\n        metadata_kg = json.load(f)\n\n    # 4. Insert all nodes\n    #    We flatten node[\"properties\"] into the node so that each key in `properties`\n    #    is stored as a direct property on the node. If you prefer to store them as a single\n    #    JSON string, see the alternative approach commented below.\n\n    with timer(logger, \"Loading metadata knowledge graph: Nodes\"):\n        for node in metadata_kg[\"nodes\"]:\n            # node[\"properties\"] must be a dict of {string_key -&gt; scalar_value}\n            # so that `SET n += $props` can distribute them as node properties.\n\n            # Example: for {\"ANumber\":144050}, this becomes n.ANumber = 144050\n            node_props = node[\"properties\"] if \"properties\" in node else {}\n\n            # If you want each node's type to be an actual Neo4j label, you can do:\n            #   create_node_cypher = \"CREATE (n:\" + node[\"type\"] + \" {id: $id}) SET n += $props\"\n            #   but that depends on your domain model.\n\n            # add project_id to node properties\n            node_props[\"project_id\"] = self.project_id\n            create_node_cypher = (\n                \"\"\"\n                CREATE (n:\"\"\"\n                + node.get(\"type\", \"Node\")\n                + \"\"\"{id: $id, type: $type})\n            SET n += $props\n            \"\"\"\n            )\n            session.run(\n                create_node_cypher,\n                id=node[\"id\"],\n                label=node.get(\"type\", \"Node\"),  # fallback empty string if no type\n                type=node.get(\"type\", \"\"),  # fallback empty string if no type\n                props=node_props,\n            )\n\n    with timer(logger, \"Loading metadata knowledge graph: Relationships\"):\n        for relation in metadata_kg[\"relationships\"]:\n            # cypher query to match the id of the start and end nodes\n            # and create a relationship between them\n            create_relationship_cypher = \"\"\"\n            MATCH (start), (end)\n            WHERE start.id = $start_id AND end.id = $end_id\n            CREATE (start)-[:RELATES_TO $props]-&gt;(end)\n            \"\"\"\n            # add project_id to relationship properties\n            relation_props = (\n                relation[\"properties\"] if \"properties\" in relation else {}\n            )\n            relation_props[\"project_id\"] = self.project_id\n            session.run(\n                create_relationship_cypher,\n                start_id=relation[\"source\"],\n                end_id=relation[\"target\"],\n                props=relation_props,\n            )\n\n    logger.info(f\"Metadata knowledge graph loaded for project {self.project_id}.\")\n    return True\n</code></pre>"},{"location":"sources/utils/neo4j_loader/#Docs2KG.utils.neo4j_loader.Neo4jTransformer.merge_entities","title":"<code>merge_entities()</code>","text":"<p>Merge entities with same label and text within the same project</p> Source code in <code>Docs2KG/utils/neo4j_loader.py</code> <pre><code>def merge_entities(self):\n    \"\"\"Merge entities with same label and text within the same project\"\"\"\n    with self.driver.session(database=self.database) as session:\n        # First, find duplicate entities (same label, text, and project)\n        find_duplicates_query = \"\"\"\n        MATCH (e1)\n        WHERE e1.text IS NOT NULL AND e1.method IS NOT NULL  // ensure it's an entity\n        WITH e1.text as text, labels(e1)[0] as label, e1.project_id as project_id,\n             collect(e1) as entities, count(*) as count\n        WHERE count &gt; 1\n        RETURN text, label, project_id, entities\n        \"\"\"\n\n        duplicates = session.run(find_duplicates_query)\n\n        for record in duplicates:\n\n            entities = record[\"entities\"]\n\n            # Keep first entity as primary\n            primary_entity = entities[0]\n            duplicate_entities = entities[1:]\n\n            # For each duplicate\n            for dup_entity in duplicate_entities:\n                # First, redirect all incoming relationships\n                session.run(\n                    \"\"\"\n                MATCH (dup) WHERE elementId(dup) = $dup_id\n                MATCH (primary) WHERE elementId(primary) = $primary_id\n                MATCH (dup)&lt;-[r]-()\n                WITH dup, r, startNode(r) as start_node, primary, properties(r) as props\n                CREATE (start_node)-[new_r:HAS_ENTITY]-&gt;(primary)\n                SET new_r = props\n                WITH dup, r\n                DELETE r\n                \"\"\",\n                    dup_id=dup_entity.element_id,\n                    primary_id=primary_entity.element_id,\n                )\n\n                # Then, redirect all outgoing relationships\n                session.run(\n                    \"\"\"\n                MATCH (dup) WHERE elementId(dup) = $dup_id\n                MATCH (primary) WHERE elementId(primary) = $primary_id\n                MATCH (dup)-[r]-&gt;()\n                WITH dup, r, endNode(r) as end_node, primary, properties(r) as props\n                CREATE (primary)-[new_r:RELATES_TO]-&gt;(end_node)\n                SET new_r = props\n                WITH dup, r\n                DELETE r\n                \"\"\",\n                    dup_id=dup_entity.element_id,\n                    primary_id=primary_entity.element_id,\n                )\n\n                # Finally, delete duplicate node\n                session.run(\n                    \"\"\"\n                MATCH (dup) WHERE elementId(dup) = $dup_id\n                DELETE dup\n                \"\"\",\n                    dup_id=dup_entity.element_id,\n                )\n\n        # Create uniqueness constraint if it doesn't exist\n        try:\n            session.run(\n                \"\"\"\n            CREATE CONSTRAINT unique_entity IF NOT EXISTS\n            FOR (e:Entity)\n            REQUIRE (e.text, e.label, e.project_id) IS UNIQUE\n            \"\"\"\n            )\n        except Exception as e:\n            # Handle older Neo4j versions or other constraint errors\n            print(f\"Warning: Could not create constraint - {str(e)}\")\n</code></pre>"},{"location":"sources/utils/neo4j_loader/#Docs2KG.utils.neo4j_loader.Neo4jTransformer.sanitize_label","title":"<code>sanitize_label(label)</code>  <code>staticmethod</code>","text":"<p>Sanitize label for Neo4j: - Replaces spaces and hyphens with underscores - Converts to uppercase - Moves any leading numbers to the end of the label</p> Source code in <code>Docs2KG/utils/neo4j_loader.py</code> <pre><code>@staticmethod\ndef sanitize_label(label: str) -&gt; str:\n    \"\"\"\n    Sanitize label for Neo4j:\n    - Replaces spaces and hyphens with underscores\n    - Converts to uppercase\n    - Moves any leading numbers to the end of the label\n    \"\"\"\n    # First sanitize special characters\n    sanitized = label.replace(\" \", \"_\").replace(\"-\", \"_\").upper()\n\n    # If label starts with a number, move leading numbers to end\n    if sanitized and sanitized[0].isdigit():\n        leading_nums = \"\"\n        i = 0\n        while i &lt; len(sanitized) and (\n            sanitized[i].isdigit() or sanitized[i] == \"_\"\n        ):\n            leading_nums += sanitized[i]\n            i += 1\n        return f\"{sanitized[i:]}{leading_nums}\" if i &lt; len(sanitized) else sanitized\n\n    return sanitized\n</code></pre>"},{"location":"sources/utils/neo4j_loader/#Docs2KG.utils.neo4j_loader.Neo4jTransformer.transform_and_load","title":"<code>transform_and_load(input_path)</code>","text":"<p>Transform and load data into Neo4j</p> Source code in <code>Docs2KG/utils/neo4j_loader.py</code> <pre><code>def transform_and_load(self, input_path: Path):\n    \"\"\"Transform and load data into Neo4j\"\"\"\n    if \"layout\" not in str(input_path):\n        logger.warning(\"Input file is not a layout knowledge graph\")\n        return\n    layout_json = json.load(open(input_path, \"r\", encoding=\"utf-8\"))\n\n    with self.driver.session(database=self.database) as session:\n        # Load metadata knowledge graph\n        self.load_metadata_kg(session)\n\n        # Create file node with unique ID\n        self.current_file_id = f\"{self.project_id}_{layout_json['filename']}\"\n        file_props = {\n            \"id\": self.current_file_id,\n            \"filename\": layout_json[\"filename\"],\n            \"project_id\": self.project_id,\n        }\n\n        session.run(\n            \"\"\"\n            CREATE (f:File $props)\n            \"\"\",\n            props=file_props,\n        )\n\n        # Reset state\n        self.header_stack = []\n\n        # Process layout structure\n        self._create_layout(session, layout=layout_json[\"data\"])\n\n        # Process entities and relations\n        for item in layout_json[\"data\"]:\n            self._process_entities(session, item)\n            self._process_relations(session, item)\n\n        # Merge duplicate entities after all data is loaded\n        self.merge_entities()\n</code></pre>"},{"location":"sources/utils/timer/","title":"Timer","text":""},{"location":"sources/utils/timer/#Docs2KG.utils.timer.timer","title":"<code>timer</code>","text":"<p>util function used to log the time taken by a part of program</p> Source code in <code>Docs2KG/utils/timer.py</code> <pre><code>class timer:\n    \"\"\"\n    util function used to log the time taken by a part of program\n    \"\"\"\n\n    def __init__(self, the_logger: Logger, message: str):\n        \"\"\"\n        init the timer\n\n        Args:\n            the_logger (Logger): logger object\n            message (str): message to be logged\n\n\n        \"\"\"\n        self.message = message\n        self.logger = the_logger\n        self.start = 0\n        self.duration = 0\n        self.sub_timers = []\n\n    def __enter__(self):\n        \"\"\"\n        context enters to start to write this\n        \"\"\"\n        self.start = time.time()\n        self.logger.info(\"Starting %s\" % self.message)\n        return self\n\n    def __exit__(self, context, value, traceback):\n        \"\"\"\n        context exit will write this\n        \"\"\"\n        self.duration = time.time() - self.start\n        self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"sources/utils/timer/#Docs2KG.utils.timer.timer.__enter__","title":"<code>__enter__()</code>","text":"<p>context enters to start to write this</p> Source code in <code>Docs2KG/utils/timer.py</code> <pre><code>def __enter__(self):\n    \"\"\"\n    context enters to start to write this\n    \"\"\"\n    self.start = time.time()\n    self.logger.info(\"Starting %s\" % self.message)\n    return self\n</code></pre>"},{"location":"sources/utils/timer/#Docs2KG.utils.timer.timer.__exit__","title":"<code>__exit__(context, value, traceback)</code>","text":"<p>context exit will write this</p> Source code in <code>Docs2KG/utils/timer.py</code> <pre><code>def __exit__(self, context, value, traceback):\n    \"\"\"\n    context exit will write this\n    \"\"\"\n    self.duration = time.time() - self.start\n    self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"sources/utils/timer/#Docs2KG.utils.timer.timer.__init__","title":"<code>__init__(the_logger, message)</code>","text":"<p>init the timer</p> <p>Parameters:</p> Name Type Description Default <code>the_logger</code> <code>Logger</code> <p>logger object</p> required <code>message</code> <code>str</code> <p>message to be logged</p> required Source code in <code>Docs2KG/utils/timer.py</code> <pre><code>def __init__(self, the_logger: Logger, message: str):\n    \"\"\"\n    init the timer\n\n    Args:\n        the_logger (Logger): logger object\n        message (str): message to be logged\n\n\n    \"\"\"\n    self.message = message\n    self.logger = the_logger\n    self.start = 0\n    self.duration = 0\n    self.sub_timers = []\n</code></pre>"}]}